---
title: 'Learning Lab 1 Case Study: Prediction'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

[First, please add your name above!]{.underline}

## Background

In the overview presentation for this learning lab, we considered five
steps in our supervised machine learning process. Those steps are
mirrored here in this case study, with the addition of a preamble step
whereby we load and process the data.

We'll focus on **machine learning**, but you'll also do a good bit of
**data wrangling**, too.

When we step back and think about how we want to use machine learning,
**predicting** is a key word. Many scholars have focused on predicting
students who are *at-risk*: of dropping a course or not succeeding in
it.

We'll dive deeper into specific examples of this in the next learning
lab, for now focusing on the mechanics of **making predictions**.

### Conceptual focus

Conceptually, we focus on prediction and how it differs from the goals
of description or explanation. We have two readings that accompany this.

The first reading is on this distinction between prediction and
description or explanation--and it is one of the most widely-read papers
in machine learning in that it articulated how machine learning differs
from other kinds of statistical models. Breiman describes the difference
in terms of *data modeling* (models for description and explanation) and
*algorithmic modeling* (what we call prediction or machine learning
models)\*

> Breiman, L. (2001). Statistical modeling: The two cultures (with
> comments and a rejoinder by the author). *Statistical Science, 16*(3),
> 199-231.
> <https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.pdf>

You'll be asked to reflect on this article later on (in the badge
activity); consider reading it now.

### Technical focus

Technically, we'll focus on the core parts of doing a machine learning
analysis in R. We'll use the {[tidymodels](https://www.tidymodels.org/)}
set of R packages (add-ons) to do so.

We'll focus on a driving question: *How well can we predict students who
are at risk of dropping a course?*

## Step 0: Loading, setting up, and wrangling data

First, let's load the packages we'll use---the {tidyverse} and
{tidymodels}. We'll also load {janitor} to help with some data cleaning
tasks.

As a tip, use the `library()` function to load these packages. After
you've done that, click the green arrow to run the code chunk. If you
see a bunch of messages (but anything labeled as an error), you are good
to go! These messages mean the packages loaded correctly.

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

We'll be using a widely-used data set in the learning analytics field:
the [Open University Learning Analytics Dataset
(OULAD)](https://analyse.kmi.open.ac.uk/open_dataset). The OULAD was
created by learning analytics researchers at the United Kingdom-based
Open University. It includes data from post-secondary learners
participation in one of several Massive Open Online Courses (called
*modules* in the OULAD).

> Kuzilek, J., Hlosta, M., & Zdrahal, Z. (2017). Open university
> learning analytics dataset. *Scientific Data, 4(*1), 1-8.
> <https://www.nature.com/articles/sdata2017171>

Please read this article.

The data can be downloaded at the above link; however, for our purposes,
they are already downloaded to the `data` sub-folder.

We'll use the `read_csv()` function to load the file. Note: we have done
some minimal processing of these files to make getting us started
easier. If you're interested in what we've done, check out the `oulad.R`
file in the `lab-1` folder.

For now, please read in the `oulad-students.csv` file. Use the
`read_csv()` function to do this, paying attention to where those files
are located relative to this case study file.

```{r}
students <- read_csv("data/oulad-students.csv")
```

You can see a description of the data
[here](https://analyse.kmi.open.ac.uk/open_dataset#description). The
students file includes three files joined together: studentInfo,
courses, and studentRegistration. Take a look at the data description to
get a sense for what variables are in which data frame.

In the chunk below, examine the data set using a function or means of
your choice (such as just *printing* the data set by typing its name or
using the `glimpse()` function). Do this in the code chunk below! Note
its dimensions --- especially how many rows it has. Add a few dashes
after the chunk with your observations.

```{r}
glimpse(students)
```

*Observations*:

-   

-   

A useful function for exploring data is `count()`; it does what it
sounds like! It counts how many times values for a variable appear.

Referring to the [data
description](https://analyse.kmi.open.ac.uk/open_dataset#description),
in the chunk below, count the number of students. Also, count the number
of courses (modules) and specific offerings (as modules can be offered
multiple times per year). Learn more about `count()`
[here](https://dplyr.tidyverse.org/reference/count.html).

```{r}
students %>% 
    count(id_student) # this many students

students %>% 
    count(code_module, code_presentation) # this many offerings
```

Another handy function is `distinct()`. This returns the unique (or
distinct) values for a specified variable. Learn more about `distinct()`
[here](https://dplyr.tidyverse.org/reference/distinct.html). Below, find
the distinct assessment IDs.

```{r, eval = FALSE}
assessments %>% 
    distinct(id_assessment) # this many unique assessments
```

We're going to do a few more steps related to data wrangling
here--noting we could also do these at later stages of our process
(namely, in the feature engineering stage).

First, let's use `mutate()` to create a dichotomous variable for whether
or not the student withdrew from the course. Here's a way we can do
this, using `if_else()` and `as.factor()`. This will be our *outcome*
variable, or the predicted variable.

```{r}
students <- students %>% 
    mutate(withdrawn = ifelse(final_result == "Withdrawn", 1, 0)) %>% # creates a dummy code
    mutate(withdrawn = as.factor(withdrawn)) # makes the variable a factor, helping later steps
```

We'll create another predictor variable for students' self-reported
disability status, but we'll do this as sa part of our machine learning
workflow (in Step 2, below).

We have one last step. Let's create one more predictor variable based on
a measure of socioeconomic resources--the index of multiple depravity
variable. The process we take here is to turn this variable that is a
character string into a number of creating a factor and then *coercing*
it to an integer. Please replace the \_\_\_ values in the code below
with the correct variable.

```{r}
students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels

```

We're now ready to proceed to the five machine learning steps.

## Step 1. Split data

-   The *training set* is used to estimate develop and compare models,
    feature engineering techniques, tune models, etc.

-   The *test set* is held in reserve until the end of the project, at
    which point there should only be one or two models under serious
    consideration. It is used as an unbiased source for measuring final
    model performance.

There are different ways to create these partitions of the data and
there is no uniform guideline for determining how much data should be
set aside for testing. The proportion of data can be driven by many
factors, including the size of the original pool of samples and the
total number of predictors.

Here, we split the data. We do so using our first {tidymodels}
function - `initial_split()`.

It is common when beginning a modeling project to [separate the data
set](https://bookdown.org/max/FES/data-splitting.html) into two
partitions. Why do we choose an 80% split (see `prop = .80` below)

This is to reserve a sufficient number of cases for testing our fitted
model later. You can change this number if you wish.

After you decide how much to set aside, the most common approach for
actually partitioning your data is to use a random sample. For our
purposes, we'll use random sampling to select 20% for the test set and
use the remainder for the training set, which are the defaults for the
{[rsample](https://tidymodels.github.io/rsample/)} package.

Additionally, since random sampling uses random numbers, it is important
to set the random number seed. This ensures that the random numbers can
be reproduced at a later time (if needed). We pick the first date on
which we may carry out this learning lab as the seed, but any number
will work!

The function `initial_split()` function from the {rsample} package takes
the original data and saves the information on how to make the
partitions. The {rsample} package has two aptly named functions for
created a training and testing data set called `training()` and
`testing()`, respectively.

Run the following code to split the data:

```{r}
set.seed(20230712)

train_test_split <- initial_split(students, prop = .80)
data_train <- training(train_test_split)
```

Go ahead and type `data_train` and `students` into the console (in
steps) to check that this data set indeed has 80% of the number of
observations as in the larger data. Do that in the chunk below:

```{r}
data_train
students
```

## Step 2: Engineer features and write down the recipe

We'll engage in a very basic feature engineering step, though we'll do
this *much* more in the next learning lab. Read more about feature
engineering [here](https://www.tmwr.org/recipes.html).

To do feature engineering, we introduce another {tidymodels} package,
[recipes](https://recipes.tidymodels.org/), which is designed to help
you prepare your data *before* training your model - in other words, to
engage in *feature engineering*.

**For now, we'll do just one feature engineering step**.

To get started, let's create a recipe for a simple logistic regression
model. Before training the model, we can use a recipe.

The
[`recipe()`function](https://recipes.tidymodels.org/reference/recipe.html) as
we used it here has two arguments:

-   A **formula**. Any variable on the left-hand side of the tilde (`~`)
    is considered the model outcome (`code`, in our present case). On
    the right-hand side of the tilde are the predictors. Variables may
    be listed by name, or you can use the dot (`.`) to indicate all
    other variables as predictors.

-   The **data**. A recipe is associated with the data set used to
    create the model. This will typically be the *training* set, so
    `data = train_data` here. Naming a data set doesn't actually change
    the data itself; it is only used to catalog the names of the
    variables and their types, like factors, integers, dates, etc.

Let's create a recipe where we predict `withdrawn` (the outcome
variable) on the basis of the `disability` and `imd_band` (predictor)
variables. We'll also add `step_dummy()`, which dummy codes the
specified variable -- here, for students' self-reported disability
status.

```{r}
my_rec <- recipe(withdrawn ~ 1, data = data_train)
```

## Step 3: Specify the model and workflow

Next, we specify the model - using the `logistic_reg()` function to set
the *model* - using `set_engine("glm")` to set the *engine* - finally,
using `set_mode("classification"))` to set the "*mode*" to
classification; this could be changed to regression for a
continuous/numeric outcome:

```{r}
# specify model
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% # generalized linear model
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression
```

## Step 4: Fit model

We will want to use our recipes created earlier across several steps as
we train and test our model. To simplify this process, we can use a
*model workflow*, which pairs a model and recipe together.

This is a straightforward approach because different recipes are often
needed for different models, so when a model and recipe are bundled, it
becomes easier to train and test *workflows*.

So, last, we'll put the pieces together - the model and recipe.

We'll use the {[workflows](https://workflows.tidymodels.org/)} package
from tidymodels to bundle our parsnip model (`my_mod`) with our first
recipe (`my_rec`).

```{r}
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

Finally, we'll fit our model.

```{r}
fitted_model <- fit(my_wf, data = data_train)
```

Importantly, here, we can look at the model. Type `fitted_model` in a
code chunk and note what you observe, focusing on the coefficients.
While we don't typically interpret the coefficients for a machine
learning model, it's important to recognize that many models *do*
produce coefficients we *could* interpret. Instead, we focus on how the
model does with respect to predicting the dependent variable.

```{r}
fitted_model
```

**Observations**

-   

Finally, we'll use the `last_fit` function, which is the key here: note
that it uses the `train_test_split` data---not just the training data.

Here, then, we fit the data *using the training data set* and evaluate
its accuracy using the *testing data set* (which is not used to train
the model).

```{r}
final_fit <- last_fit(fitted_model, train_test_split)
```

Type `final_fit` below; this is the final, fitted model---one that can
be interpreted further in the next step!

```{r}
final_fit
```

You may see a message/warning above or when you examine `final_fit`; you
can safely ignore that.

## Step 5: Interpret accuracy

Run the code below to examine the predictions for the *test* split of
data. Note that the row ID is in the output below, but this doesn't
correspond one-one to the ID variables used in the presentation/Shiny.

```{r}
# collect test split predictions
final_fit %>%
    collect_predictions()

collect_predictions(final_fit) %>% 
    conf_mat(.pred_class, withdrawn)
```

This is our first set of real output! Note two things:

1.  `.pres_class`: This is the *predicted* code\
2.  `code`: This is the known *code*

When these are **the same**, the model predicted the code *correctly*;
when they aren't the same, the model was incorrect.

Importantly, we can *summarize* across all of these codes. One way to do
this is straightforward; how many of the codes were the same, as in the
following chunk of code:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, withdrawn) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == withdrawn) # create a new variable, correct, telling us when the model was and was not correct
```

You may notice some of the rows may be missing values. This is because
there were some missing values in the `imd_band` variable, and for this
machine learning algorithm (the generalized linear model), missing
values result in row-wise deletion.

That's helpful, but there's one more step we can take -- counting up the
values of `correct`:

```{r}
final_fit %>% 
    collect_predictions() %>% # see test set predictions
    select(.pred_class, withdrawn) %>% # just to make the output easier to view 
    mutate(correct = .pred_class == withdrawn) %>% # create a new variable, correct, telling us when the model was and was not correct
    tabyl(correct)
```

Let's interpret the above. If the value of `correct` is `TRUE` when the
predicted and known code are the same, what does the `percent` column
tell us? Add one or more notes to the dashes below:

-   

A short-cut to the above is below; **we'll use this short-cut from here
forward**, having seen how accuracy is calculated.

```{r}
final_fit %>% 
    collect_metrics()

final_fit %>% 
    collect_predictions() %>% 
    count(.pred_class)
```

How accurate was our predictive model? Consider how well our model would
have done by chance alone -- what would the accuracy be in that case
(with the model predicting withdrawn one-half of the time)?

```{r}
students_ss %>% 
    count(withdrawn)

students_ss %>% 
    mutate(prediction = sample(c(0, 1), nrow(students_ss), replace = TRUE)) %>% 
    mutate(correct = if_else(prediction == 1 & withdrawn == 1 |
               prediction == 0 & withdrawn == 0, 1, 0)) %>% 
    tabyl(correct)
```

Curiously, randomly picking a 0 (did not withdraw) or a 1 (withdrawn)
will *always* lead to around a 50% accuracy, regardless of how many
observations are actually associated with a 0 or a 1.

**Observations**

-   

-   

Let's step back a bit. How well *could* we do if we include more data?
And how *useful* could such a model be in the real world? We'll dive
into these questions more over the forthcoming learning labs.

That's it for now; the core parts of machine learning are used in the
above steps you took; what we'll do after this leaning lab only adds
nuance and complexity to what we've already done.

## 🧶 Knit & Check ✅

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
