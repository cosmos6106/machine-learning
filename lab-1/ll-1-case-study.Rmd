---
title: 'Learning Lab 1 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(20240706) # so the results are readily reproducible
```

## 1. PREPARE

We will be using data from the IPEDS.

## 2. WRANGLE

#### **ðŸ‘‰ Your Turn** **â¤µ**

```{r}

```

## 3. EXPLORE

```{r}

```

## 4. MODEL

### Step 1: Explore a range of solutions

We *could* estimate a single solution. Please use the
`estimate_profiles()` function with the data in `data_for_lpa`,
specifying the number of profiles as 3. For help, consult the
[help](https://data-edu.github.io/tidyLPA/reference/estimate_profiles.html)
file.

```{r}
data_for_lpa %>% 
    estimate_profiles(n_profiles = 3)
```

We can see output for *a single model fit*. Let's take a look at what
the estimates are. We can do so in several ways. In particular, we can
pipe the output above to either `plot_profiles()` or `get_estimates()`.
Let's add `plot_profiles(add_line = TRUE)` to the above code, copying
all three lines into the code chunk below.

```{r}
data_for_lpa %>% 
    estimate_profiles(n_profiles = 3) %>% 
    plot_profiles(add_line = TRUE)
```

What we are looking at are the estimates for the *means* of the three
profiles (or, in LPA's terminology, classes). For instance, for the
first class (represented with the color red), we can see how the profile
mean is higher for some variables (relative to the means for the other
classes), and lower for others. We'll go into much more depth on
intepreting a profile solution shortly.

### Step 2: Select a solution

Done, right? Not quite. Notice that above, we specified that the number
of profiles was 3. How do we know that 3 is best? This is a key decision
that we as the individual or group analyzing the data must make.
Fortunately, there are tools to help us.

To do so, we'll examine a range of different profile solutions.

Let's try the `compare_solutions()` function, which does just that. The
`estimate_profiles()` function can, helpfully, estimate more than one
set of profiles at once --- here, those with 1 through 8 profiles.

#### **ðŸ‘‰ Your Turn** **â¤µ**

To do so, please run `estimate_profiles` like we have above,

```{r}



```

We can interpret the fit statistic that is provided---the BIC, with more
negative values indicating a better model fit---and see which model is
best. In addition, we can use a process that considers several fit
indices, the *analytic hierarchy process*. Sometimes, what the BIC and
the analytic hierarchy process suggest diverge, as is the case here. In
such a case, it can be helpful to interrogate several candidate
solutions in a fine-grained way.

#### **ðŸ‘‰ Your Turn** **â¤µ**

We can examine many fit statistics for candidate models by replacing
`compare_solutions()` with `get_fit()`. You can read about each fit
index
[here](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html).
Please run that code, below.

```{r}

```

We can also plot several possible solutions; again, please replace
`compare_solutions()`, this time with `plot_profiles(add_line = TRUE)`:

```{r}
data_for_lpa %>%
    estimate_profiles(1:4) %>% 
    plot_profiles(add_line = TRUE)
```

How do we know which solution to be *the* solution we interpret and
present? We need to consider several different factors, including a) the
BIC and other individual fit indices, b) the analytic hierarchy output,
c) considerations of parsimony, and d) considerations related to the
interpretability of the profiles.

### Step 3: Select and interpret a solution

To do so, please run `estimate_profiles` like we have above,

```{r}
data_for_lpa %>%
    estimate_profiles(1:7) %>% 
    compare_solutions()
```

We can interpret the fit statistic that is provided---the BIC, with more
negative values indicating a better model fit---and see which model is
best. In addition, we can use a process that considers several fit
indices, the *analytic hierarchy process*. Sometimes, what the BIC and
the analytic hierarchy process suggest diverge, as is the case here. In
such a case, it can be helpful to interrogate several candidate
solutions in a fine-grained way.

#### [Your Turn]{style="color: green;"} â¤µ

We can examine many fit statistics for candidate models by replacing
`compare_solutions()` with `get_fit()`. You can read about each fit
index
[here](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html).
Please run that code, below.

```{r}

```

We can also plot several possible solutions; again, please replace
`compare_solutions()`, this time with `plot_profiles(add_line = TRUE)`,
adding the varying number of profiles you wish to plot.

#### [Your Turn]{style="color: green;"} â¤µ

```{r}

```

How do we know which solution to be *the* solution we interpret and
present? We need to consider several different factors, including a) the
BIC and other individual fit indices, b) the analytic hierarchy output,
c) considerations of parsimony, and d) considerations related to the
interpretability of the profiles.

### Step 3: Select and interpret a solution

*Which solution do you thinks is best?* Given that the BIC indicates a
three profile model, and given how the model with four profiles does not
appear to yield a substantially distinct fourth profile, it seems
reasonable to prefer three profiles over four. But, the analytic
hierarchy process suggested a single profile model. Is a model with
fewer profiles potentially merited? When comparing the three- and
two-profile models, it seems that one profile is more distinguishable
(based on the mean estimates) than the other two. For this reason, let's
consider moving forward with a two-profile solution, recognizing that
this data is somewhat messy, and the way forward is not wholly clear!

To do so, run `estimate_profiles()`, specifying two profiles, and
assigning the output the name `two_profile_solution`.

#### **ðŸ‘‰ Your Turn** **â¤µ**

```{r}

```

Let's plot this solution using `plot_profiles(add_line = TRUE)`

```{r}
plot_profiles(two_profile_solution, add_line = TRUE)
```

Typically, we'd name these solutions. Let's try to interpret and name
these based upon their mean values:

-   Profile 1:
-   Profile 2:

```{r}
two_profile_solution <- estimate_profiles(data_for_lpa, n_profiles = 2)
```

Let's plot this solution using `plot_profiles(add_line = TRUE)`

```{r}
plot_profiles(two_profile_solution, add_line = TRUE)
```

Typically, we'd name these solutions. Let's try to interpret and name
these based upon their mean values:

-   Profile 1:
-   Profile 2:

We still aren't done! Recall from the presentation how we discussed
Computational Grounded Theory.

In this approach, an exploratory approach such as Latent Profile
Analysis is carried out not as the end point, but as a first step in
understanding the data. The second step involves examining the data with
the output from the first step as a guide. In this step, you as the
human can interrogate and make sense of the output of the first step.

Let's first get the profiles for each row of our data set.

```{r}
data_for_two_profile_solution <- 
    get_data(two_profile_solution) %>% # get the classes for each row of data
    select(Class) %>% # let's select just the class (profile) variable
    mutate(segment_id = unique(transcript$segment_id)) # this assigns the segment IDs back to the data, so we can join the transcript data later on
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Then, let's *bind* together the profiles assigned to each chunk with the
original data. Please use
[`bind_rows()`](https://dplyr.tidyverse.org/reference/bind.html),
providing both `data_for_lpa` and `data_for_two_profile_solution`
together and assigning the output the name `combined_data`.

```{r}

```

Now, let's take a look at the data using `View()` the data frame we
create next, `data_to_view`. *Don't* write this in a code chunk;
instead, just view the data frame you jsut created by typing the code
into the console (as `View()` can cause issues when it comes time to
knit --- unless it is commented!).

```{r}
data_to_view <- left_join(transcript, combined_data) # this joins the transcript and combined data, so we can see which segment is associated with which profile, or class
# View(data_to_view)
```

```{r}
data_for_two_profile_solution <- 
    get_data(two_profile_solution) %>% # get the classes for each row of data
    select(Class) %>% # let's select just the class (profile) variable
    mutate(segment_id = unique(transcript$segment_id)) # this assigns the segment IDs back to the data, so we can join the transcript data later on
```

#### [Your Turn]{style="color: green;"} â¤µ

Then, let's *bind* together the profiles assigned to each chunk with the
original data. Please use
[`bind_rows()`](https://dplyr.tidyverse.org/reference/bind.html),
providing both `data_for_lpa` and `data_for_two_profile_solution`
together and assigning the output the name `combined_data`.

```{r}

```

Now, let's take a look at the data using `View()` the data frame we
create next, `data_to_view`. *Don't* write this in a code chunk;
instead, just view the data frame you just created by typing the code
into the console (as `View()` can cause issues when it comes time to
knit --- unless it is commented!).

```{r}
data_to_view <- left_join(transcript, combined_data) # this joins the transcript and combined data, so we can see which segment is associated with which profile, or class
# View(data_to_view)
```

## 5. COMMUNICATE

For now, let's keep our task relatively straightforward. How well do our
names seem to characterize what is going on in a particular segment? Add
a few notes below. Again, focus on what you would communicate about this
analysis to a general audience.

-   ADD YOUR RESPONSE HERE

-   ADD YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
