---
title: 'Learning Lab 3 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```

## Background

After interpreting our last model, it is easy to think we can do better.
How? In this learning lab, we will answer this question will building a
better model.

Feature engineering is a rich topic in machine learning research,
including in the learning analytics and educational data mining
communities.

Consider research on online learning and the work of Li et al. (2020)
and Rodriguez et al. (2021). In these two studies, digital *log-trace
data*, data generated through users' interactions with digital
technologies, was used to study elements of the theoretical frame of
*self-regulated learning* and how it related to students' achievement.
Notably, the authors took several steps to prepare the data so that it
could be validly interpreted as measures of students' self-regulated
learning. In short, we need to process the data from contexts such as
online classes to use them in analyses. Citations and links to these
papers follow.

> Li, Q., Baker, R., & Warschauer, M. (2020). Using clickstream data to
> measure, understand, and support self-regulated learning in online
> courses. The Internet and Higher Education, 45, 100727.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-2/li-et-al-2020-ihe.pdf>
>
> Rodriguez, F., Lee, H. R., Rutherford, T., Fischer, C., Potma, E., &
> Warschauer, M. (2021, April). Using clickstream data mining techniques
> to understand and support first-generation college students in an
> online chemistry course. In LAK21: 11th International Learning
> Analytics and Knowledge Conference (pp. 313-322).
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-2/rodriguez-et-al-2021-lak.pdf>

The same is true here in the context of machine learning. In a different
context, the work of Gobert et al. (2013) is a great example of using
data from educational simulations. Salmeron-Majadas provides an example
of feature engineering using mouse-click data. Last, we note that there
are methods that intended to automated the process of feature
engineering (Bosch et al., 2021), though such processes are not
necessarily interpretable and they usually require some degree of
tailoring to your particular context.

> Gobert, J. D., Sao Pedro, M., Raziuddin, J., & Baker, R. S. (2013).
> From log files to assessment metrics: Measuring students' science
> inquiry skills using educational data mining. Journal of the Learning
> Sciences, 22(4), 521-563.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/gobert-et-al-2013-jls.pdf>
>
> Bosch, N. (2021). AutoML Feature Engineering for Student Modeling
> Yields High Accuracy, but Limited Interpretability. Journal of
> Educational Data Mining, 13(2), 55-79.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/bosch-et-al-2021-jedm.pdf>

Even after feature engineering, machine learning approaches can often
(but not always) be improved by choosing a more sophisticated model
type. Note how we used a regression model in the first two case studies;
here, we explore a considerably more sophisticated model, a random
forest.

Choosing a more sophisticated model adds some complexity to the
modeling. In addition to using feature engineering in a way akin to how
we did in the last case study, Bertolini et al. (2021) use tuning
parameters to improve the performance of their predictive model.

> Bertolini, R., Finch, S. J., & Nehm, R. H. (2021). Enhancing data
> pipelines for forecasting student performance: integrating feature
> selection with cross-validation. *International Journal of Educational
> Technology in Higher Education, 18*(1), 1-23.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/bertolini-et-al-2021-ijethe.pdf>

Our driving question is: **How much of a difference does a more complex
model make?** Looking back to our predictive model from LL1, we can see
that our accuracy was around xx. Can we improve on that?

While answering this question, we focus not only on estimating, but also
on tuning a complex model. The data we use is, again, from the #NGSSchat
community on Twitter, as in doing so we can compare the performance of
this tuned, complex model to the initial model we used in the first case
study.

> Baker, R. S., Esbenshade, L., Vitale, J., & Karumbaiah, S. (2023).
> Using Demographic Data as Predictor Variables: a Questionable Choice.
> *Journal of Educational Data Mining*, *15*(2), 22-52.

## Step 0: Loading and setting up

First, let's load the packages we'll use---the familiar {tidyverse} and
several others focused on modeling. Like in earlier learning labs, click
the green arrow to run the code chunk.

#### [Your Turn]{style="color: green;"} ⤵

Please add to the chunk below code to load three packages we've used in
both LL1 and LL2 - tidyverse, tidymodels, and here.

```{r}
library(janitor)
library(tidyverse)
library(tidymodels)
library(vip) # a new package we're adding for variable importance measures
library(ranger) # this is needed for the random forest algorithm
```

Next, we'll load the processed data.

```{r}
assessments <- read_csv("lab-3/data/oulad-assessments.csv")

code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))

# students and assessments 
students_and_assessments <- read_csv("lab-3/data/oulad-students-and-assessments.csv")

```

```{r}
# log-data
interactions <- read_csv("lab-3/data/oulad-interactions.csv")

interactions %>% 
    count(activity_type)

interactions %>% 
    ggplot(aes(x = log(sum_click))) +
    geom_histogram()

interactions %>% 
    ggplot(aes(x = date)) +
    geom_histogram()

interactions %>% 
    count(id_site, code_module, code_presentation)

## ---

code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))

interactions_joined <- interactions %>% 
    left_join(code_module_dates) # join the data based on course_module and course_presentation

interactions_filtered <- interactions_joined %>% 
    filter(date < quantile_cutoff_date) # filter the data so only assignments before the cutoff date are included

## ---------------------------------------------------------------------------------------------------

interactions_summarized <- interactions_filtered %>% 
    group_by(id_student, code_module, code_presentation) %>% 
    summarize(sum_clicks = sum(sum_click),
              sd_clicks = sd(sum_click), 
              max_clicks = max(sum_click))

interactions_summarized %>% 
    ggplot(aes(x = sum_clicks)) +
    geom_histogram()

interactions_summarized_activity <- interactions_filtered %>% 
    group_by(id_student, code_module, code_presentation, activity_type) %>% 
    summarize(sum_clicks = sum(sum_click))

interactions_summarized_activity %>% 
    ggplot(aes(x = sum_clicks)) +
    geom_histogram() +
    facet_wrap(~activity_type)

interactions_slopes <- interactions_filtered %>% 
    group_by(id_student, code_module, code_presentation) %>% 
    nest() %>% 
    mutate(model = map(data, ~lm(sum_click ~ 1 + date, data = .x) %>% 
                           tidy)) %>% 
    unnest(model)

interactions_slopes %>% 
    filter(term == "date")

students_assessments_and_interactions <- left_join(students_and_assessments, 
                                                   interactions_summarized)
```

```{r}
students_assessments_and_interactions <- students_assessments_and_interactions %>% 
    mutate(pass = as.factor(pass))
```

## Step 1. Split data



We treat this step relatively minimally as we have now carried out a
step very similar to this in LL1 and LL2; return to the case study for
those (especially LL1) for more on data splitting. Note that we carry
out the *k*-folds cross-validation process introduced in LL2. Consider -
like there - setting a different value for *v* (*k*) as you think is
appropriate.

Here's a key difference! Pay careful attention to this next line of
code, which sets the groundwork for *k*-folds cross-validation. Note
that in the function below (run `?vfold_cv` to see more), the letter *v*
is used instead of *k*, though they share a meaning, as the
documentation notes).

```{r}
kfcv <- vfold_cv(data_train) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
kfcv
```

#### [Your Turn]{style="color: green;"} ⤵

Above, we split the data into 10 different folds. Change the number of
folds from 10 to 20 by changing the value of v; 10 is simply the
default. For help, run `?vfold_cv` to get a hint.

```{r}
kfcv <- vfold_cv(data_train) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
kfcv
```

#### [Your Turn]{style="color: green;"} ⤵

You do have one step that is your turn! Please add the code for setting
up the k-folds cross-validation.

```{r}
set.seed(20230712)

train_test_split <- initial_split(students_assessments_and_interactions , prop = .50, strata = "pass")
data_train <- training(train_test_split)
kfcv <- vfold_cv(data_train) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
data_test <- testing(train_test_split)
```

Here's a key difference! Pay careful attention to this next line of
code, which sets the groundwork for *k*-folds cross-validation. Note
that in the function below (run `?vfold_cv` to see more), the letter *v*
is used instead of *k*, though they share a meaning, as the
documentation notes).

## Step 2: Engineer features and write down the recipe

Here, we'll carry out several feature engineering steps.

#### [Your Turn]{style="color: green;"} ⤵

Read about [possible steps](https://www.tmwr.org/recipes.html) and see
more about how the following five feature engineering steps below work.
Like in the first learning lab, this is the step in which we set the
recipe.

-   `step_normalize(all_numeric_predictors())`
-   `step_nzv(all_predictors())`
-   `step_novel(all_nominal_predictors())`
-   `step_dummy(all_nominal_predictors())`
-   `step_impute_knn(all_predictors(), all_outcomes())`

In Step 0, we noted how we added three variables as potential features.
Here, we carry out two feature engineering steps we have carried out
before - standardizing the numeric variables (to have a mean equal to 0
and a standard deviation equal to 1) and dropping any features with
near-zero variance. Consider adding other feature engineering steps -
perhaps the step you carried out complete the badge requirements for
LL2.

```{r,}
my_rec <- recipe(pass ~ disability +
                     date_registration + 
                     gender +
                     code_module +
                     mean_weighted_score +
                     sum_clicks, 
                 data = data_train) %>% 
    step_dummy(disability) %>% 
    step_dummy(gender) %>%  
    step_dummy(code_module) %>% 
    step_impute_knn(sum_clicks) %>% 
    step_impute_knn(date_registration)
```

## Step 3: Specify the model and workflow

Next, we specify the model and workflow, using the same engine *but a
different engine and mode*, here, regression for a *continuous outcome*.
Specifically, we use:

-   using the `linear_reg()` function to set the *model*
-   using `set_engine("glm")` to set the *engine*
-   finally, using `set_mode("regression"))`

## Step 3: Specify recipe, model, and workflow

There are several steps that are different from the past learning labs
here.

-   using the `random_forest()` function to set the *model* as a random
    forest
-   using `set_engine("ranger", importance = "impurity")` to set the
    *engine* as that provided for random forests through the {ranger}
    package; we also add the `importance = "impurity"` line to be able
    to interpret a particular variable importance metric (impurity)
    specific to random forest models
-   finally, using `set_mode("classification"))` as we are again
    predicting categories (transactional and substantive conversations
    taking place through #NGSSchat)

```{r panel-chunk-3, echo = TRUE, eval = TRUE}
# specify model
my_mod <-
    rand_forest() %>% 
    set_engine("ranger", importance = "impurity") %>% # random
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression

# specify workflow
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

## Step 4: Fit model

Note that here we use the `kfcv` data. We'll run that in the next chunk.

We can ignore the warnings and messages we see.

```{r}
fitted_model_resamples <- fit_resamples(my_wf, resamples = kfcv,
                              control = control_grid(save_pred = TRUE)) # this allows us to inspect the predictions

fitted_model_resamples %>%
    collect_metrics()
```

```{r, warning = FALSE}
class_metrics <- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap) # add probs?

fitted_model <- fit(my_wf, data_train)
final_fit <- last_fit(fitted_model, train_test_split, metrics = class_metrics)

final_fit %>% 
    collect_metrics()
```

We can see that `final_fit` is for a single fit: a random forest with
the best performing tuning parameters trained with the *entire* training
set of data to predict the values in our (otherwise not used/"spent")
testing set of data.

## Step 5: Interpret accuracy

Last, we can interpret the accuracy of our tuned model.

```{r fit-stats}
collect_predictions(final_fit)

collect_predictions(final_fit) %>% 
    conf_mat(.pred_class, pass)

collect_metrics(final_fit
```

Interpreting these - apart from `accuracy` - may present some
challenges. First, we can focus on the accuracy - around 89% (0.886).
Accuracy and the others are defined below.

-   *Accuracy*: For the known codes, what percentage of the predictions
    are correct

-   *Cohen's K*: Same as accuracy, while account for the base rate of
    (chance) agreement

-   *Sensitivity (AKA recall)*: Among the true "positives", what
    percentage are classified as "positive"?

-   *Specificity*: Among the true "negatives", what percentage are
    classified as "negative"?

-   *ROC AUC*: For different levels of the threshold (for making a
    predicting), what is the sensitivity (predicted true, true positive)
    and specificity (predicted negative, true negative) rate?

You'll have the chance to interpret these further in the badge for this
learning lab.

One last note - we may be interested to see which variables were most
importance. We can do this with the following.

```{r}
final_fit %>% 
    pluck(".workflow", 1) %>%   
    extract_fit_parsnip() %>% 
    vip(num_features = 10)
```

### 🧶 Knit & Check ✅

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
