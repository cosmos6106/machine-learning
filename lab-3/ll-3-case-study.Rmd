---
title: 'Learning Lab 3 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
```


1.  We use a very different data set, on from *online science classes*
    that involves a variety of variables types
2.  We focus on *feature engineering*, a key step in which we prepare
    variables for inclusion in our machine learning models
3.  We use resampling to evaluate the effectiveness of the feature
    engineering steps

Feature engineering is a rich topic in machine learning research,
including in the learning analytics and educational data mining
communities.

Consider research on online learning and the work of Li et al. (2020)
and Rodriguez et al. (2021). In these two studies, digital *log-trace
data*, data generated through users' interactions with digital
technologies, was used to study elements of the theoretical frame of
*self-regulated learning* and how it related to students' achievement.
Notably, the authors took several steps to prepare the data so that it
could be validly interpreted as measures of students' self-regulated
learning. In short, we need to process the data from contexts such as
online classes to use them in analyses. Citations and links to these
papers follow.

> Li, Q., Baker, R., & Warschauer, M. (2020). Using clickstream data to
> measure, understand, and support self-regulated learning in online
> courses. The Internet and Higher Education, 45, 100727.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-2/li-et-al-2020-ihe.pdf>
>
> Rodriguez, F., Lee, H. R., Rutherford, T., Fischer, C., Potma, E., &
> Warschauer, M. (2021, April). Using clickstream data mining techniques
> to understand and support first-generation college students in an
> online chemistry course. In LAK21: 11th International Learning
> Analytics and Knowledge Conference (pp. 313-322).
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-2/rodriguez-et-al-2021-lak.pdf>

The same is true here in the context of machine learning. In a different
context, the work of Gobert et al. (2013) is a great example of using
data from educational simulations. Salmeron-Majadas provides an example
of feature engineering using mouse-click data. Last, we note that there
are methods that intended to automated the process of feature
engineering (Bosch et al., 2021), though such processes are not
necessarily interpretable and they usually require some degree of
tailoring to your particular context.

> Gobert, J. D., Sao Pedro, M., Raziuddin, J., & Baker, R. S. (2013).
> From log files to assessment metrics: Measuring students' science
> inquiry skills using educational data mining. Journal of the Learning
> Sciences, 22(4), 521-563.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/gobert-et-al-2013-jls.pdf>

> Salmeron-Majadas, S., Baker, R. S., Santos, O. C., & Boticario, J. G.
> (2018). A machine learning approach to leverage individual keyboard
> and mouse interaction behavior from multiple users in real-world
> learning scenarios. IEEE Access, 6, 39154-39179.
> <https://ieeexplore.ieee.org/iel7/6287639/8274985/08416736.pdf>

> Bosch, N. (2021). AutoML Feature Engineering for Student Modeling
> Yields High Accuracy, but Limited Interpretability. Journal of
> Educational Data Mining, 13(2), 55-79.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/bosch-et-al-2021-jedm.pdf>

Even after feature engineering, machine learning approaches can often
(but not always) be improved by choosing a more sophisticated model
type. Note how we used a regression model in the first two case studies;
here, we explore a considerably more sophisticated model, a random
forest.

Choosing a more sophisticated model adds some complexity to the
modeling. Notably, more complex models have *tuning parameters* - parts
of the model that are not estimated from the data. In addition to using
feature engineering in a way akin to how we did in the last case study,
Bertolini et al. (2021) use tuning parameters to improve the performance
of their predictive model.

> Bertolini, R., Finch, S. J., & Nehm, R. H. (2021). Enhancing data
> pipelines for forecasting student performance: integrating feature
> selection with cross-validation. *International Journal of Educational
> Technology in Higher Education, 18*(1), 1-23.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-3/bertolini-et-al-2021-ijethe.pdf>

Our driving question is: **How much of a difference does a more complex
model make?** Looking back to our predictive model from LL1, we can see
that our accuracy was around 87%: 0.872, more specifically. Can we
improve on that?

While answering this question, we focus not only on estimating, but also
on tuning a complex model. The data we use is, again, from the #NGSSchat
community on Twitter, as in doing so we can compare the performance of
this tuned, complex model to the initial model we used in the first case
study.

## Step 0: Loading and setting up

First, let's load the packages we'll use---the familiar {tidyverse} and
several others focused on modeling. Like in earlier learning labs, click
the green arrow to run the code chunk.

#### [Your Turn]{style="color: green;"} ⤵

Please add to the chunk below code to load three packages we've used in
both LL1 and LL2 - tidyverse, tidymodels, and here.

```{r}
library(tidyverse)
library(tidymodels)
library(vip) # a new package we're adding for variable importance measures
library(ranger) # this is needed for the random forest algorithm
```

Next, we'll load the processed data.

```{r}
students <- read_csv("data/oulad-students.csv")
assessments <- read_csv("data/oulad-assessments.csv")

students <- students %>% 
    mutate(withdrawn = ifelse(final_result == "Withdrawn", 1, 0)) %>% # creates a dummy code
    mutate(withdrawn = as.factor(withdrawn)) # makes the variable a factor, helping later steps

students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels

students %>% 
    select(date_registration) # just taking a look at this variable
```

```{r}
code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))

assessments <- assessments %>% 
    left_join(code_module_dates)

assessments_filtered <- assessments %>% 
    filter(date < quantile_cutoff_date)

assessments_summarized <- assessments_filtered %>% 
    mutate(weighted_score = score * weight) %>% 
    group_by(id_student) %>% 
    summarize(mean_weighted_score = mean(weighted_score)) %>% 
    arrange(id_student)
```

## Step 1. Split data

We treat this step relatively minimally as we have now carried out a
step very similar to this in LL1 and LL2; return to the case study for
those (especially LL1) for more on data splitting. Note that we carry
out the *k*-folds cross-validation process introduced in LL2. Consider -
like there - setting a different value for *v* (*k*) as you think is
appropriate.

#### [Your Turn]{style="color: green;"} ⤵

You do have one step that is your turn! Please add the code for setting
up the k-folds cross-validation.

```{r}
set.seed(20220712)
train_test_split <- initial_split(students, prop = .80)
data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
kfcv
```


Here's a key difference! Pay careful attention to this next line of
code, which sets the groundwork for *k*-folds cross-validation. Note
that in the function below (run `?vfold_cv` to see more), the letter *v*
is used instead of *k*, though they share a meaning, as the
documentation notes).

## Step 2: Engineer features and write down the recipe

Here, we'll carry out several feature engineering steps.

#### [Your Turn]{style="color: green;"} ⤵

Read about [possible steps](https://www.tmwr.org/recipes.html) and see
more about how the following five feature engineering steps below work.
Like in the first learning lab, this is the step in which we set the
recipe.

-   `step_normalize(all_numeric_predictors())`
-   `step_nzv(all_predictors())`
-   `step_novel(all_nominal_predictors())`
-   `step_dummy(all_nominal_predictors())`
-   `step_impute_knn(all_predictors(), all_outcomes())`



In Step 0, we noted how we added three variables as potential features.
Here, we carry out two feature engineering steps we have carried out
before - standardizing the numeric variables (to have a mean equal to 0
and a standard deviation equal to 1) and dropping any features with
near-zero variance. Consider adding other feature engineering steps -
perhaps the step you carried out complete the badge requirements for
LL2.

```{r,}
my_rec <- recipe(withdrawn ~ disability + imd_band +
                     date_registration + 
                     gender + highest_education + 
                     mean_weighted_score, data = data_train) %>% 
    step_dummy(disability) %>% 
    step_dummy(gender) %>% 
    step_dummy(highest_education)
```

## Step 3: Specify the model and workflow

Next, we specify the model and workflow, using the same engine *but a
different engine and mode*, here, regression for a *continuous outcome*.
Specifically, we use:

-   using the `linear_reg()` function to set the *model*
-   using `set_engine("glm")` to set the *engine*
-   finally, using `set_mode("regression"))`

## Step 3: Specify recipe, model, and workflow

There are several steps that are different from the past learning labs
here.

-   using the `random_forest()` function to set the *model* as a random
    forest
-   using `set_engine("ranger", importance = "impurity")` to set the
    *engine* as that provided for random forests through the {ranger}
    package; we also add the `importance = "impurity"` line to be able
    to interpret a particular variable importance metric (impurity)
    specific to random forest models
-   finally, using `set_mode("classification"))` as we are again
    predicting categories (transactional and substantive conversations
    taking place through #NGSSchat)

```{r panel-chunk-3, echo = TRUE, eval = TRUE}
# specify model
my_mod <-
    rand_forest(mtry = tune(), # this specifies that we'll take steps later to tune the model
                min_n = tune(),
                trees = tune()) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("classification")

# specify workflow
my_wf <-
    workflow() %>%
    add_model(my_mod) %>% 
    add_recipe(my_rec)
```

## Step 4: Fit model

Here, things become different once again. We'll follow a grid method to
specify two tuning parameters, the number of predictor variables that
are randomly sampled for each split in the tree (`mtry`) and the number
of data points required to execute a split (`min_n`). `size` refers to
how many distinct combinations of the tuning parameters will be
returned. 10 is a relatively small number - we can imagine a much larger
number of combinations of the `mtry` and `min_n` hyperparameters - but
it should give us a sense of what parameters lead to the best
performance.

These next two functions are used to get a sense of what the values for
`mtry` and `min_n` should be based on the dimensions or range of the
values of the variables in the data.

```{r panel-chunk-4, echo = TRUE, eval = TRUE}
# specify tuning grid
finalize(mtry(), data_train)
finalize(min_n(), data_train)
finalize(trees(), data_train)
```

#### [Your Turn]{style="color: green;"} ⤵

We then use these values in the `grid_max_entropy()` function below;
replace the `xx` values below with the *minimum* and *maximum* value
provided by the `mtry` and `min_n` variables, above. You can see that
`tree_grid` is simply a combination of values of the hyperparameters.

```{r}
tree_grid <- grid_max_entropy(mtry(range(1, 17)),
                              min_n(range(2, 40)),
                              trees(range(1, 2000)),
                              size = 10)
```

Now, we're ready to fit the model. Note - this can take some time, the
longest of any function we've run so far, as we're estimating a) as many
models as there are folds (*v =* 10 as default) and b) distinct
combinations of hyperparameters (`size = 10` as default).


## Step 4: Fit model

Note that here we use the `kfcv` data. We'll run that in the next chunk.

We can ignore the warnings and messages we see.

```{r, warning = FALSE}
fitted_model_resamples <- fit_resamples(my_wf, resamples = kfcv,
                              control = control_grid(save_pred = TRUE)) # this allows us to inspect the predictions
```

```{r, warning = FALSE}
# fit model with tune_grid()

fitted_model <- my_wf %>% 
    tune_grid(
        resamples = kfcv,
        grid = tree_grid,
        metrics = metric_set(roc_auc, accuracy, kap, sensitivity, specificity, precision)
    )
```

Here comes some further additional steps. This next step is key
technically and conceptually - we're examining the best tuning
parameters *ranked by their predictive accuracy*.

```{r}
# examine best set of tuning parameters; repeat?
show_best(fitted_model, n = 10, metric = "accuracy")
```

This function simply indicates that you want to use the best of the sets
of tuning parameters examined though the code in the above chunk -
literally the first row.

```{r}
# select best set of tuning parameters
best_tree <- fitted_model %>%
    select_best(metric = "accuracy")
```

Next, we'll finalize workflow with best set of tuning parameters and
then fit the model on the training data.

```{r}
final_wf <- my_wf %>% 
    finalize_workflow(best_tree)

final_fit <- final_wf %>% 
    last_fit(train_test_split, metrics = metric_set(roc_auc, accuracy, kap, sensitivity, specificity, precision)
             )
```

We can see that `final_fit` is for a single fit: a random forest with
the best performing tuning parameters trained with the *entire* training
set of data to predict the values in our (otherwise not used/"spent")
testing set of data.

```{r}
final_fit
```

## Step 5: Interpret accuracy

Last, we can interpret the accuracy of our tuned model.

```{r fit-stats}
# fit stats
final_fit %>%
    collect_metrics()
```

Interpreting these - apart from `accuracy` - may present some
challenges. First, we can focus on the accuracy - around 89% (0.886).
Accuracy and the others are defined below.

-   *Accuracy*: For the known codes, what percentage of the predictions
    are correct

-   *Cohen's K*: Same as accuracy, while account for the base rate of
    (chance) agreement

-   *Sensitivity (AKA recall)*: Among the true "positives", what
    percentage are classified as "positive"?

-   *Specificity*: Among the true "negatives", what percentage are
    classified as "negative"?

-   *ROC AUC*: For different levels of the threshold (for making a
    predicting), what is the sensitivity (predicted true, true positive)
    and specificity (predicted negative, true negative) rate?

You'll have the chance to interpret these further in the badge for this
learning lab.

One last note - we may be interested to see which variables were most
importance. We can do this with the following.

```{r}
final_fit %>% 
    pluck(".workflow", 1) %>%   
    extract_fit_parsnip() %>% 
    vip(num_features = 10)
```

### 🧶 Knit & Check ✅

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
