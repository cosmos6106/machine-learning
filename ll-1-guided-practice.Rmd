---
title: 'Learning Lab 1 Guided Practice'
author: "Dr. Joshua Kellogg"
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## step 0: prep

```{r}
library(tidyverse)
library(here)
library(tidymodels)
library(vip)
library(ranger)

d <- read_rds(here("data", "ngsschat-data.rds"))

codes <- read_csv(here("data", "ngsschat-qualitative-codes.csv"))

codes <- codes %>% 
    select(id_string = ID, code = Code)

dd <- d %>% 
    left_join(codes) %>% 
    filter(!is.na(code)) %>% 
    filter(code != "OT" & code != "RT" & code != "TB")

ddd <- dd %>% 
    select(favorite_count, retweet_count, followers_count, friends_count, statuses_count, display_text_width, 
           # screen_name, text,
           # created_at, account_created_at,
           code, id_string) %>% 
    filter(!is.na(favorite_count)) %>% 
    group_by(id_string)

ddd <- ddd %>% 
    summarize(mean_favorite_count = mean(favorite_count),
              mean_retweet_count = mean(retweet_count),
              mean_status_count = mean(statuses_count),
              mean_followers_count = mean(followers_count),
              mean_friends_count = mean(friends_count),
              sum_display_text_width = sum(display_text_width),
              n = n()) %>% 
    left_join(distinct(dd, id_string, code)) %>% 
    select(-id_string)

ddd %>% write_csv("ngsschat-data-joined.csv")
```

## step 1: split 

```{r}
train_test_split <- initial_split(ddd, prop = .70)

data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train)
```

## step 2: pre-processing/feature engineering

```{r}
my_rec <- recipe(code ~ ., data = ddd) %>% 
    # add_role(screen_name, text, new_role = "text variables") %>% # this can be any string
    # step_novel(all_nominal_predictors()) %>% 
    step_normalize(all_numeric_predictors()) %>%
    # step_dummy(all_nominal_predictors()) %>% 
    step_nzv(all_predictors())
    # step_impute_knn(all_predictors(), all_outcomes())
```

## step 3: set up the model, workflow, and tuning grid

```{r}
# specify model
rf_mod_many <-
    rand_forest(mtry = tune(),
                min_n = tune()) %>%
    set_engine("ranger", importance = "impurity") %>%
    set_mode("classification") %>%  # note this
    step_impute_knn(all_predictors(), all_outcomes())

# specify workflow
rf_wf_many <-
    workflow() %>%
    add_model(rf_mod_many) %>% 
    add_recipe(my_rec)

# specify tuning grid
finalize(mtry(), data_train)
finalize(min_n(), data_train)

tree_grid <- grid_max_entropy(mtry(range(1, 8)),
                              min_n(range(2, 40)),
                              size = 5)
```

## 4: fit model with tune_grid

```{r}
tree_res <- rf_wf_many %>% 
    tune_grid(
        resamples = kfcv,
        grid = tree_grid,
        metrics = metric_set(roc_auc, accuracy, kap, sensitivity, specificity, precision)
    )

# examine best set of tuning parameters; repeat?
show_best(tree_res, n = 10)
```

## 5: evaluate

```{r}
# examine best set of tuning parameters; repeat?
show_best(tree_res, n = 10)

# select best set of tuning parameters
best_tree <- tree_res %>%
    select_best()

# finalize workflow with best set of tuning parameters
final_wf <- rf_wf_many %>% 
    finalize_workflow(best_tree)

# fit split data (separately)
final_fit <- final_wf %>% 
    last_fit(train_test_split, metrics = metric_set(roc_auc, accuracy, kap, 
                                                    sensitivity, specificity, precision))

# fit stats
final_fit %>%
    collect_metrics()

# variable importance plot
final_fit %>% 
    pluck(".workflow", 1) %>%   
    pull_workflow_fit() %>% 
    vip(num_features = 10)

# test set predictions
final_fit %>%
    collect_predictions() 

lr_auc <- final_fit %>% 
    collect_predictions() %>% 
    roc_curve(code, .pred_0)

autoplot(lr_auc)
```