---
title: 'Learning Lab 4 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output: 
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(20230630) # so the results are readily reproducible
```

## 1. PREPARE

While the past three case studies have involved *already-coded* data,
sometimes, we do not have something that we can consider to be coded
data---a dependent variable. But, machine learning does offer us way
forward in such cases, but not through the supervised machine learning
methods we've been using until this point. Instead, we can use
*unsupervised* methods.

The driving question for this learning lab is: *What if we do not have
training data?*

We answer this question in the context of *multimodal* data, or data
from multiple channels, including not only the kinds of digital trace
data and survey data, but also data from audio and video channels. This
kind of data is often rich, but hard to interpret at a granular level.

We'll be using audio data from mathematics classrooms collected as a
part of Dyer's [*Learning Through Teaching* (LTT)
project](https://www.proquest.com/openview/01af3361c531b12ea3caeda8b85b31fa/1?pq-origsite=gscholar&cbl=18750).
In this project, hundres of hours of video and audio were collected from
high school and middle school classrooms. The specific data set we are
using is transcribed data (via
[Whisper](https://github.com/bnosac/audio.whisper)) that is analyzed for
the different affective or emotional sentiment it evidences. The goal of
the study is to try to identify moments in the audio data that suggest
negative affect on the part of students in the context of learning
mathematics.

How can we do this? We'll use the process of Latent Profile Analysis to
try to identify how students' affect is expressed. We'll interpret this
output through a *Computational Grounded Theory* approach, one that
takes the output of techniques like Latent Profile Analysis as a
starting point for subsequent qualitative analyses.

Please consult two papers to get a good handle on the methodological
background:

First, Commeford et al. (2022) estimated profiles of college-levels
instructors' teaching practices, specifically their teaching practices
relating to active learning. They did so using Latent Profile Analysis
-- and [the tidyLPA package in
R](https://data-edu.github.io/tidyLPA/index.html) that we'll be using.

> Commeford, K., Brewe, E., & Traxler, A. (2022). Characterizing active
> learning environments in physics using latent profile analysis.
> Physical Review Physics Education Research, 18(1), 010113.
> <https://github.com/laser-institute/essential-readings/blob/main/machine-learning/ml-lab-4/commeford-et-al-2022-per.pdf>

Second, please read a paper that uses Computational Grounded
Theory---here, in the context of students' written responses to in-class
questions:

> Rosenberg, J. M., & Krist, C. (2021). Combining machine learning and
> qualitative methods to elaborate students' ideas about the generality
> of their model-based explanations. *Journal of Science Education and
> Technology, 30*, 255-267.

One note for this learning lab. LPA can be a finnicky method; the best
practices regarding model selection are often difficult to achieve in
practice. We'll do our best working with the information we have - as we
often have to do in practice. If publishing a paper, we can present
these issues in our manuscript in a transparent manner.

## 2. WRANGLE

First, let's load the packages we'll use---the familiar {tidyverse} and
the {tidyLPA package}, as well as {tidytext} to do some basic text
analysis.

#### **ðŸ‘‰ Your Turn** **â¤µ**

```{r}

```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Next, please read in the
`r-processed-transcript`.csv`file, naming it`transcript\`.

```{r}

```

## 3. EXPLORE

#### **ðŸ‘‰ Your Turn** **â¤µ**

Please take a look at the data using a technique of your choice. Note
the number of rows and the variables and their types.

```{r}
transcript %>% 
    glimpse()
```

Note how the `transcript_text` variable consists of transcripted *turns*
of speech (not individual words), corresponding roughly to what an
individual says in a conversation.

We'll next do some analyses over several chunks to create the variables
for the emotional expression evidenced in the transcripted data. Given
the focus of this learning lab on Latent Profile Analysis, we'll treat
this aspect in a more limited way. Please consider joining the first
text analysis learning lab to gain a lot more context and to see in far
greater detail how we can work with textual data in R.

```{r}
nrc <- get_sentiments("nrc") # this access sentiment

transcript <- transcript %>% 
    select(group, index, start, end, duration, transcript_text) # select the variables we'll be using

transcript %>% 
    unnest_tokens(word, transcript_text) %>% # this changes the data to be in "long" form, with each row consisting of individual words in the transcript data
    left_join(nrc, relationship = "many-to-many") %>% # ignore warnings
    count(sentiment)
```

Since each turn is of a different length, next, we'll create 30-second
chunks in the data.

```{r}
chunk_size <- 45  # chunk duration in seconds
start_point <- transcript$start %>% as.integer() %>% pluck(1) # find the starting point of the time stamps
end_point <- transcript$end %>% as.integer() %>% pluck(nrow(transcript)) # find the ending point of the time stamps

transcript$start <- as.integer(transcript$start)
transcript$end <- as.integer(transcript$end)

# Create a new variable for the chunks
transcript$segment_id <- cut(transcript$start, breaks = seq(from = start_point, to = end_point, by = chunk_size))
```

Finally, we'll do some additional processing. A *lot* of this is new,
and not important to have a mastery of for this learning lab. To learn
more, check out the text mining labs!

```{r}
number_of_words_per_segment <- transcript  %>% 
    unnest_tokens(word, transcript_text) %>% 
    count(segment_id) %>% 
    rename(words_per_segment = n)

data_for_lpa <- transcript  %>% 
    unnest_tokens(word, transcript_text) %>% # create a one-word-per-row structure
    left_join(nrc, relationship = "many-to-many") %>% # join the sentiment data
    count(segment_id, sentiment) %>% # count the number of words assigned to each emotional expression
    spread(sentiment, n) %>% # change the data to be in wide form
    janitor::clean_names() %>% # make the names easier to type
    left_join(number_of_words_per_segment) %>% # join the number of words per segment
    reframe(pct_fear = fear / words_per_segment, # create summary variables, dividing each sentiment score by the nubmer of the words in each segment
            pct_joy = joy / words_per_segment,
            pct_anticipation = anticipation / words_per_segment,
            pct_disgust = disgust / words_per_segment,
            pct_sadness = sadness / words_per_segment,
            pct_surprise = surprise / words_per_segment,
            pct_trust = trust / words_per_segment) %>% 
    mutate_if(is.numeric, replace_na, 0) # replace NA values with 0s
```

We're now ready to proceed to modeling.

## 4. MODEL

### Step 1: Explore a range of solutions

We *could* estimate a single solution. Please use the
`estimate_profiles()` function with the data in `data_for_lpa`,
specifying the number of profiles as 3. For help, consult the
[help](https://data-edu.github.io/tidyLPA/reference/estimate_profiles.html)
file.

```{r}
data_for_lpa %>% 
    estimate_profiles(n_profiles = 3)
```

We can see output for *a single model fit*. Let's take a look at what
the estimates are. We can do so in several ways. In particular, we can
pipe the output above to either `plot_profiles()` or `get_estimates()`.
Let's add `plot_profiles(add_line = TRUE)` to the above code, copying
all three lines into the code chunk below.

```{r}
data_for_lpa %>% 
    estimate_profiles(n_profiles = 3) %>% 
    plot_profiles(add_line = TRUE)
```

What we are looking at are the estimates for the *means* of the three
profiles (or, in LPA's terminology, classes). For instance, for the
first class (represented with the color red), we can see how the profile
mean is higher for some variables (relative to the means for the other
classes), and lower for others. We'll go into much more depth on
intepreting a profile solution shortly.

### Step 2: Select a solution

Done, right? Not quite. Notice that above, we specified that the number
of profiles was 3. How do we know that 3 is best? This is a key decision
that we as the individual or group analyzing the data must make.
Fortunately, there are tools to help us.

To do so, we'll examine a range of different profile solutions.

Let's try the `compare_solutions()` function, which does just that. The
`estimate_profiles()` function can, helpfully, estimate more than one
set of profiles at once --- here, those with 1 through 8 profiles.

#### **ðŸ‘‰ Your Turn** **â¤µ**

To do so, please run `estimate_profiles` like we have above,

```{r}

```

We can interpret the fit statistic that is provided---the BIC, with more
negative values indicating a better model fit---and see which model is
best. In addition, we can use a process that considers several fit
indices, the *analytic hierarchy process*. Sometimes, what the BIC and
the analytic hierarchy process suggest diverge, as is the case here. In
such a case, it can be helpful to interrogate several candidate
solutions in a fine-grained way.

#### **ðŸ‘‰ Your Turn** **â¤µ**

We can examine many fit statistics for candidate models by replacing
`compare_solutions()` with `get_fit()`. You can read about each fit
index
[here](https://data-edu.github.io/tidyLPA/articles/Introduction_to_tidyLPA.html).
Please run that code, below.

```{r}

```

We can also plot several possible solutions; again, please replace
`compare_solutions()`, this time with `plot_profiles(add_line = TRUE)`:

```{r}
data_for_lpa %>%
    estimate_profiles(1:4) %>% 
    plot_profiles(add_line = TRUE)
```

How do we know which solution to be *the* solution we interpret and
present? We need to consider several different factors, including a) the
BIC and other individual fit indices, b) the analytic hierarchy output,
c) considerations of parsimony, and d) considerations related to the
interpretability of the profiles.

### Step 3: Select and interpret a solution

*Which solution do you thinks is best?* Given that the BIC indicates a
three profile model, and given how the model with four profiles does not
appear to yield a substantially distinct fourth profile, it seems
reasonable to prefer three profiles over four. But, the analytic
hierarchy process suggested a single profile model. Is a model with
fewer profiles potentially merited? When comparing the three- and
two-profile models, it seems that one profile is more distinguishable
(based on the mean estimates) than the other two. For this reason, let's
consider moving forward with a two-profile solution, recognizing that
this data is somewhat messy, and the way forward is not wholly clear!

#### **ðŸ‘‰ Your Turn** **â¤µ**

To do so, run `estimate_profiles()`, specifying two profiles, and
assigning the output the name `two_profile_solution`.

```{r}

```

Let's plot this solution using `plot_profiles(add_line = TRUE)`

```{r}
plot_profiles(two_profile_solution, add_line = TRUE)
```

Typically, we'd name these solutions. Let's try to interpret and name
these based upon their mean values:

-   Profile 1:
-   Profile 2:

We still aren't done! Recall from the presentation how we discussed
Computational Grounded Theory.

In this approach, an exploratory approach such as Latent Profile
Analysis is carried out not as the end point, but as a first step in
understanding the data. The second step involves examining the data with
the output from the first step as a guide. In this step, you as the
human can interrogate and make sense of the output of the first step.

Let's first get the profiles for each row of our data set.

```{r}
data_for_two_profile_solution <- 
    get_data(two_profile_solution) %>% # get the classes for each row of data
    select(Class) %>% # let's select just the class (profile) variable
    mutate(segment_id = unique(transcript$segment_id)) # this assigns the segment IDs back to the data, so we can join the transcript data later on
```

#### **ðŸ‘‰ Your Turn** **â¤µ**

Then, let's *bind* together the profiles assigned to each chunk with the
original data. Please use
[`bind_rows()`](https://dplyr.tidyverse.org/reference/bind.html),
providing both `data_for_lpa` and `data_for_two_profile_solution`
together and assigning the output the name `combined_data`.

```{r}

```

Now, let's take a look at the data using `View()` the data frame we
create next, `data_to_view`. *Don't* write this in a code chunk;
instead, just view the data frame you jsut created by typing the code
into the console (as `View()` can cause issues when it comes time to
knit --- unless it is commented!).

```{r}
data_to_view <- left_join(transcript, combined_data) # this joins the transcript and combined data, so we can see which segment is associated with which profile, or class
# View(data_to_view)
```

## 5. COMMUNICATE

For now, let's keep our task relatively straightforward. How well do our
names seem to characterize what is going on in a particular segment? Add
a few notes below. Again, focus on what you would communicate about this
analysis to a general audience.

-   ADD YOUR RESPONSE HERE

-   ADD YOUR RESPONSE HERE

### ðŸ§¶ Knit & Check âœ…

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
