---
title: 'Learning Lab 1 Case Study'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, please add your name above!

In the overview presentation for this learning lab, we considered five steps in our supervised machine learning process. Those steps are mirrored here in this case study---with the addition of a preamble step whereby we load and process the data. 

Our driving question is: Can we predict something we would have coded by hand?

We use the #NGSSchat data set as the context in which we answer this question. See the network analysis of #NGSSChat (Rosenberg et al., 2020) that used the coding frame from van Bommel et al. (2020) for the transactional or substantive nature of social media-based conversations. Notably, Rosenberg et. al. coded _a lot_ of data by hand, and it would be quite convenient if the coding could be automated through supervised machine learning methods. Though this case study is tied to the #NGSSchat data, you can consider how qualitative coding you or colleagues have done could be automated in a similar manner. In short, again, can we predict something we would, heretofore, have coded by hand?

Such an approach is common in education. Maestrales et al. (2021) present a great example of how coded data and a coding frame can be "scaled up", here for assessment-related purposes. Consider reading through this paper, noting how the authors tuned the performance of their supervised machine learning classification models. They used the Constructed Response Classifier (https://beyondmultiplechoice.org/) to do so; this tool has been widely-used in undergraduate, disciplinary-based research settings.

Conceptually, we focus on prediction and how it differs from the goals of description or explanation.

## Step 0: Loading and setting up

First, let's load the packages we'll use---the familiar {tidyverse} and several others focused on modeling. Like in earlier learning labs, click the green arrow to run the code chunk.

```{r}
library(tidyverse)
library(here)
library(tidymodels)
library(vip)
library(ranger)
```

Next, we'll load two data sources, one with the tweets, the other with our qualitative codes. 

*Note*: We created a means of visualizing the threads to make coding them easier; that's here and it provides a means of seeing what the raw data is like: https://jmichaelrosenberg.shinyapps.io/ngsschat-shiny/

```{r}
d <- read_rds(here("data", "ngsschat-data.rds"))

codes <- read_csv(here("data", "ngsschat-qualitative-codes.csv"))
```

We'll do some processing of the data here. This could seem like a step to skip, but it's quite important as perhaps the most important part of any machine learning model is the data that goes into it. 

Here, read the comments after each line of code, and then run the code chunk to proceed. 

```{r}
codes <- codes %>% 
    select(id_string = ID, code = Code) # this changes the variables names in the codes data frame to be the esame as in the tweets

dd <- d %>% 
    left_join(codes) %>% # join the two files together
    filter(!is.na(code)) %>% 
    filter(code != "OT" & code != "RT" & code != "TF") # here, because TF (transformational) codes are so rare, we exclude them, as well as OT (off-topic) and RT (retweet) tweets and those missing a code

ddd <- dd %>% 
    select(favorite_count, retweet_count, followers_count, friends_count, statuses_count, display_text_width, 
           code, id_string) %>% # select the variables we'll use for our supervised machine learning model
    filter(!is.na(favorite_count)) %>% 
    group_by(id_string) %>% # group the tweets by thread and calculate summary variables
    summarize(mean_favorite_count = mean(favorite_count), # this and the next are means; this could, though, be a sum; it represents the average number of favorites each tweet in the thread received
              mean_retweet_count = mean(retweet_count), # how many retweets each tweet received
              sum_display_text_width = sum(display_text_width), # this is a variable for the length of the tweet; sum seems more sensible than mean
              n = n()) %>% # the number of tweets in the thread
    left_join(distinct(dd, id_string, code)) %>%  # here, we join back the codes, as we lost them in the summary step
    select(-id_string)
```

#### [Your Turn]{style="color: green;"} â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

In the chunk below, examine the prepared data using a function or means of your choice. 

Note its dimensions --- especially how many rows it has. Add a few dashes after the chunk with your observations.

```{r}

```

## Step 1. Split data

Here, we split the data. Why do we choose an 80% split? This is to reserve a sufficient number of cases for testing our fitted model later. You can change this number if you wish.

```{r}
train_test_split <- initial_split(ddd, prop = .80)
data_train <- training(train_test_split)
```

## Step 2: Engineer features

We'll engage in some basic feature engineering. 

#### [Your Turn]{style="color: green;"} â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Before running the code below, can you add a feature engineering step to the code above?

Read more about feature engineering here and get ideas for adding a step: https://www.tmwr.org/recipes.html

```{r}
my_rec <- recipe(code ~ ., data = ddd) %>% 
    step_normalize(all_numeric_predictors()) %>%
    step_nzv(all_predictors())
```

## Step 3: Specify recipe, model, and workflow

Here, we specify one of the simplest supervised machine learning models for _classification_, a logistic regression. We do this by:

- using the `logistic_reg()` function to set the *model*
- using `set_engine("glm")` to set the *engine*
- finally, using `set_mode("classification"))` to set the "*mode* to classification; this could be changed to regression for a continuous/numeric outcome

```{r}
# specify model
rf_mod_many <-
    logistic_reg() %>% 
    set_engine("glm") %>%
    set_mode("classification")
```

Last, we'll put the pieces together - the model and recipe.

```{r}
rf_wf_many <-
    workflow() %>%
    add_model(rf_mod_many) %>% 
    add_recipe(my_rec)
```

## Step 4: Fit model

Finally, we'll fit the model, using our training data to do so. 

`last_fit` is the key function here - note that it uses the `train_test_split` data---not just the training data. Here, we fit the data _using the training data set_ and evaluate its accuracy using the _testing data set_ (which is not used to train the model).

```{r}
fitted_model <- fit(rf_wf_many, data = data_train)

final_fit <- last_fit(fitted_model, train_test_split,
               metrics = metric_set(roc_auc, accuracy, kap, sensitivity, specificity, precision))
```

#### [Your Turn]{style="color: green;"} â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Type `final_fit` below; this is the final, fitted model---one that can be interpreted further in the next step!

```{r}

```

## Step 5: Interpret accuracy

#### [Your Turn]{style="color: green;"} â¤µ {style="font-style: normal; font-variant-caps: normal; letter-spacing: normal; orphans: auto; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: auto; word-spacing: 0px; -webkit-tap-highlight-color: rgba(26, 26, 26, 0.3); -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px; text-decoration: none; caret-color: rgb(0, 0, 0); color: rgb(0, 0, 0);"}

Run the code below to calculate a _confusion matrix_:

```{r}
final_fit$.predictions[[1]] %>% 
    conf_mat(.pred_class, code)
```

### ðŸ§¶ Knit & Check âœ…

Congratulations - you've completed the Machine Learning Learning Lab 1 Guided Practice! Consider moving on to the independent practice next.