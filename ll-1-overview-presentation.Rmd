---
  title: "SNA Lab 4: Statistical Inference & Network Models"
  subtitle: "Test"
  author: "**Dr. Shaun Kellogg**"
  institute: "LASER Institute"
  date: '`r format(Sys.time(), "%B %d, %Y")`'
  output:
    xaringan::moon_reader:
      css:
       - default
       - css/laser.css
       - css/laser-fonts.css
      lib_dir: libs                        # creates directory for libraries
      seal: false                          # false: custom title slide
      nature:
        highlightStyle: default         # highlighting syntax for code
        highlightLines: true               # true: enables code line highlighting 
        highlightLanguage: ["r"]           # languages to highlight
        countIncrementalSlides: false      # false: disables counting of incremental slides
        ratio: "16:9"                      # 4:3 for standard size,16:9
        slideNumberFormat: |
         <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
         </div>
---
class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```
```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`

---
# Agenda

.pull-left[
## Part 1: Core Concepts
- Prediction versus explanation
- Train and test set
]

.pull-right[

## Part 2: R Code-Along
- Estimating a logistic regression model
- Interpreting the model in terms of its predictive accuracy
]


.panelset[
.panel[.panel-name[1]

---

# How do I select a model?

One general principle is to **start with the simplest useful model** and to _build toward
more complex models as helpfuL_.

This principle applies in multiple ways:

- To choose an algorithm, start with simpler models that you can efficiently use and understand
- To carry out feature engineering, understand your predictors well by starting with a subset
- To tune an algorithm, start with a relatively simple set of tuning parameters

This isn't just for beginners or those of us in education; [most spam filters use Support Vector Machines (and used Naive Bayes until recently)](https://vas3k.com/blog/machine_learning/) due to their combination of effectiveness and efficiency "in production."

---

# Super complex?

- Nothing too much, apart from computing power, time, and concerns of 
- A "check" on your work is your predictions on _test_ set data
  - *Train data*: Coded/outcome data that you use to train ("estimate") your model
  - *Validation data<sup>1</sup>*: Data you use to select a particular algorithm
  - *Test ("hold-out") data*: Data that you do not use in any way to train your data

- An important way to achieve good performance with test data is to balance between the inherent _bias_ in your algorithm and the _variance_ in the predictions of your algorithm; this is referred to as the **bias-variance** trade-off of _all_ models

.footnote[
[1] not always/often used, for reasons we'll discuss later

]

class: clear, inverse, center, middle

# Machine learning demo: Regression

---

# Overview of ML regression

- We are interested in predicting a _continuous_ outcome; accordingly, our predictions are _continuous_
- Metrics for how well the model performed include:
  - Root Mean Square Error (RMSE): can be interpreted as the _average_ difference between $y$ and $\hat{y}$
  - Mean Absolute Error (MAE): can be interpreted as the _median_ difference between $y$ and $\hat{y}$
  - $R^2$: Proportion of variance explained (used descriptively)

---

**Split data (into train, test, and tuning sets sets)**

```{r panel-chunk-1, fig.show='hide', eval = FALSE}
train_test_split <- initial_split(d, prop = .70)

data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train)
```
]

.panel[.panel-name[2]

**Specify recipe, model, and workflow**
 
```{r panel-chunk-2, fig.show='hide', eval = FALSE}
sci_rec <- recipe(final_grade ~ ., data = d) %>% 
    ...

rf_mod_many <-rand_forest(mtry = tune(),
              min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf_many <- workflow() %>%
  add_model(rf_mod_many) %>% 
  add_recipe(sci_rec)
```
]

.panel[.panel-name[3]

**Estimate models and determine tuning parameters' values**

```{r panel-chunk-4, fig.show='hide', eval = FALSE}
tree_res <- rf_wf_many %>% 
  tune_grid(
    resamples = kfcv,
    grid = tree_grid,
    metrics = metric_set(rmse, mae, rsq)
  )

# select best set of tuning parameters
best_tree <- tree_res %>%
  select_best()

# finalize workflow with best set of tuning parameters
final_wf <- rf_wf_many %>% 
  finalize_workflow(best_tree)
```
]

.panel[.panel-name[4]

**Evaluate metrics and predictions**

```{r panel-chunk-5, fig.show='hide', eval = FALSE}
# fit split data (separately)
final_fit <- final_wf %>% 
  last_fit(train_test_split, metrics = metric_set(rmse, mae, rsq))

# fit stats
final_fit %>%
  collect_metrics()
```
]
]

---
class: clear, center

## .font130[.center[**Thank you!**]]
<br/>
.center[<img style="border-radius: 80%;" src="img/jr-cycling" height="200px"/><br/>**Dr. Joshua Rosrenberg**<br/><mailto:jmrosenberg@utk.edu>]
