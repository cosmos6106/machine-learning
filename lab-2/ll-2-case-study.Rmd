---
title: 'Learning Lab 2 Case Study: Feature Engineering'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This case study is similar to the first, but it differs in three key
ways:


Our driving question for this case study is: **How much do new
predictors improve the prediction quality?**

We use a data set of many online classes to answer this question. To
answer it, we will engage in several feature engineering steps.

## Step 0: Loading and setting up

Like in the first learning lab, we'll first load several packages.

```{r, load packages}
library(tidyverse)
library(tidymodels)
```

Like in the code-along for the overview presentation, let's take a look
at the data and do some processing of it.

We'll load the students file together; you'll write code to read the assessments file. Please assign the name assessments to the loaded assessments file.

```{r}
students <- read_csv("data/oulad-students.csv")
assessments <- read_csv("data/oulad-assessments.csv")
```

```{r}
students <- students %>% 
    mutate(withdrawn = ifelse(final_result == "Withdrawn", 1, 0)) %>% # creates a dummy code
    mutate(withdrawn = as.factor(withdrawn)) # makes the variable a factor, helping later steps

students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels
```

Let's take a look at how many unique assessments there are, using the `count()` function. Let's also examine the number of different *types* of assessments.

```{r}
assessments %>% 
    distinct(id_assessment) # this many unique assessments

assessments %>% 
    count(assessment_type)
```

We might be interested in how many assessments there are per course.

```{r}
assessments %>% 
    count(assessment_type, code_module, code_presentation)

assessments %>% 
    select(date_submitted, date) %>% 
    mutate(delta_from_date = date_submitted - date)

assessments %>% 
    count(date) %>% 
    ggplot(aes(x = date, y = n)) +
    geom_point()

assessments %>% 
    summarize(mean_date = mean(date, na.rm = TRUE),
              median_date = median(date, na.rm = TRUE),
              sd_date = sd(date, na.rm = TRUE))

assessments %>% 
    summarize(min_date = min(date, na.rm = TRUE),
              max_date = max(date, na.rm = TRUE))

assessments %>% 
    summarize(quantile_date = quantile(date, na.rm = TRUE))

code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))

assessments <- assessments %>% 
    left_join(code_module_dates)

assessments_filtered <- assessments %>% 
    filter(date < quantile_cutoff_date)

assessments_summarized <- assessments_filtered %>% 
    mutate(weighted_score = score * weight) %>% 
    group_by(id_student) %>% 
    summarize(mean_weighted_score = mean(weighted_score)) %>% 
    arrange(id_student)
```

```{r}
students <- students %>% 
    left_join(assessments_summarized)
```

## Step 1. Split data

-   The *training set* is used to estimate develop and compare models,
    feature engineering techniques, tune models, etc.

-   The *test set* is held in reserve until the end of the project, at
    which point there should only be one or two models under serious
    consideration. It is used as an unbiased source for measuring final
    model performance.

There are different ways to create these partitions of the data and
there is no uniform guideline for determining how much data should be
set aside for testing. The proportion of data can be driven by many
factors, including the size of the original pool of samples and the
total number of predictors.

Here, we split the data. We do so using our first {tidymodels}
function - `initial_split()`.

It is common when beginning a modeling project to [separate the data
set](https://bookdown.org/max/FES/data-splitting.html) into two
partitions. Why do we choose an 80% split (see `prop = .80` below)

This is to reserve a sufficient number of cases for testing our fitted
model later. You can change this number if you wish.

After you decide how much to set aside, the most common approach for
actually partitioning your data is to use a random sample. For our
purposes, we'll use random sampling to select 20% for the test set and
use the remainder for the training set, which are the defaults for the
{[rsample](https://tidymodels.github.io/rsample/)} package.

Additionally, since random sampling uses random numbers, it is important
to set the random number seed. This ensures that the random numbers can
be reproduced at a later time (if needed). We pick the first date on
which we may carry out this learning lab as the seed, but any number
will work!

The function `initial_split()` function from the {rsample} package takes
the original data and saves the information on how to make the
partitions. The {rsample} package has two aptly named functions for
created a training and testing data set called `training()` and
`testing()`, respectively.

Run the following code to split the data:

```{r}
set.seed(20230712)

train_test_split <- initial_split(students, prop = .80)
data_train <- training(train_test_split)
```

Go ahead and type `data_train` and `students` into the console (in steps) to
check that this data set indeed has 80% of the number of observations as
in the larger data. Do that in the chunk below:

```{r}
data_train
students
```

## Step 2: Engineer features and write down the recipe

We'll engage in a very basic feature engineering step, though we'll do
this *much* more in the next learning lab. Read more about feature
engineering [here](https://www.tmwr.org/recipes.html).

To do feature engineering, we introduce another {tidymodels} package,
[recipes](https://recipes.tidymodels.org/), which is designed to help
you prepare your data *before* training your model - in other words, to
engage in *feature engineering*. That's all we'll say about this now;
we'll dive into feature engineering in the third learning lab.

**For now, we'll just use the variables as they are -- we'll do *no*
feature engineering at this stage.**

To get started, let's create a recipe for a simple logistic regression
model. Before training the model, we can use a recipe.

The
[`recipe()`function](https://recipes.tidymodels.org/reference/recipe.html)Â as
we used it here has two arguments:

-   A **formula**. Any variable on the left-hand side of the tilde (`~`)
    is considered the model outcome (`code`, in our present case). On
    the right-hand side of the tilde are the predictors. Variables may
    be listed by name, or you can use the dot (`.`) to indicate all
    other variables as predictors.

-   The **data**. A recipe is associated with the data set used to
    create the model. This will typically be the *training* set, so
    `data = train_data` here. Naming a data set doesn't actually change
    the data itself; it is only used to catalog the names of the
    variables and their types, like factors, integers, dates, etc.

Let's create a recipe where we predict `withdrawn` (the outcome
variable) on the basis of the `disability` and `imd_band` (predictor)
variables. We'll add several other variables, including the date registration variable we used in the last learning lab, gender and highest education levels, and students' assessment scores through 1/4 of the course.

```{r}
my_rec <- recipe(withdrawn ~ disability + imd_band +
                     date_registration + 
                     gender + highest_education + 
                     mean_weighted_score, data = data_train) %>% 
    step_dummy(disability) %>% 
    step_dummy(gender) %>% 
    step_dummy(highest_education)
```

## Step 3: Specify the model and workflow

Next, we specify the model - using the `logistic_reg()` function to set
the *model* - using `set_engine("glm")` to set the *engine* - finally,
using `set_mode("classification"))` to set the "*mode*" to
classification; this could be changed to regression for a
continuous/numeric outcome:

```{r}
# specify model
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>% # generalized linear model
    set_mode("classification") # since we are predicting a dichotomous outcome, specify classification; for a number, specify regression
```

## Step 4: Fit model

We will want to use our recipes created earlier across several steps as
we train and test our model. To simplify this process, we can use a
*model workflow*, which pairs a model and recipe together.

This is a straightforward approach because different recipes are often
needed for different models, so when a model and recipe are bundled, it
becomes easier to train and test *workflows*.

So, last, we'll put the pieces together - the model and recipe.

We'll use the {[workflows](https://workflows.tidymodels.org/)} package
from tidymodels to bundle our parsnip model (`my_mod`) with our first
recipe (`my_rec`).

```{r}
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

Finally, we'll fit our model.

```{r}
fitted_model <- fit(my_wf, data = data_train)
```

Importantly, here, we can look at the model. Type `fitted_model` in a
code chunk and note what you observe, focusing on the coefficients.
While we don't typically interpret the coefficients for a machine
learning model, it's important to recognize that many models *do*
produce coefficients we *could* interpret. Instead, we focus on how the
model does with respect to predicting the dependent variable.

```{r}
fitted_model
```

**Observations**

-   

Finally, we'll use the `last_fit` function, which is the key here: note
that it uses the `train_test_split` data---not just the training data.

Here, then, we fit the data *using the training data set* and evaluate
its accuracy using the *testing data set* (which is not used to train
the model).

```{r}
final_fit <- last_fit(fitted_model, train_test_split)
```

## Step 5: Interpret accuracy

```{r}
collect_metrics(final_fit)
```

What did we get as output? Let's take a look at the metrics. This is
critical to understanding how and why we use k-folds cross validation.
Each of the rows below represents the accuracy (in the `.estimate`
column) for each of the 20 folds that we used to train our model; our
model was fit 20 times, and accuracy was calculated separately for each
of these times. Next, we'll summarize these.

Recall our definition of the Root Mean Squared Error (RMSE) - it is the
*square root* of the mean of the squared error, or difference between
the predicted and known *y* variables (here, students' final grade).
Since this is the square root of a statistic that is squared, its
interpretation can be considerably simplified: **RMSE can be interpreted
as the average error, or difference between the predicted and known *y*
variables (here, students' final grade)**. This, along with the Mean
Squared Error (MSE), are the most common metrics of predictive accuracy
for a numeric outcome such as students' final grade. See more about fit
metrics for numeric/continuous outcomes (those utilized in a
*regression* mode)
[here](https://bradleyboehmke.github.io/HOML/process.html#regression-models).
The goal is to minimize both the RMSE and MSE.

Note that the common R-squared measure (`rsq` in the output) can also be
interpreted. Though helpful descriptively, it has less useful as a
measure of the predictive effectiveness of a trained model, and it
should generally not be used to select between competing model
specifications.

```{r}
fitted_model_resamples %>% # **edit: this needs to be fitted_model_resamples** 
    unnest(.metrics) %>% 
    filter(.metric == "rmse") # we also get another metric, the RSQ; we focus just on RMSE for nwo
```

Running the code below calculates the *mean* of the metrics we inspected
in the previous chunk. Focus on the **mean** variable for the accuracy
metric. This can be interpreted in the precise same was as our accuracy
measure we calculated in learning lab 1 - this is the percentage of
students the model correctly classified as passing or not passing the
course.

```{r}
# fit stats
fitted_model_resamples %>%
    collect_metrics()
```

We can imagine trying out many different sets of features (engineered in
different ways). So long as we evaluate the accuracy using the
resampling method used above, we can repeat this process as needed.
Then, we can carry out a process like that in the first learning lab -
fitting the model not using the different *folds* obtained through the
`kfcv` function, but rather using the **entire training data set**.

```{r, warning = FALSE}
fitted_model <- fit(my_wf, data_train)
```

Then, we can use the model to predict students passing (or not passing)
using our testing data *that we have not used for any purpose until this
point* --- and interpret that model. This output is suggestive to us of
how the model would perform on new data, as this testing data set has
not been used to make any decisions about the feature engineering.

```{r}
final_fit <- last_fit(fitted_model, train_test_split)
```

```{r}
collect_metrics(final_fit)
```

Last, we can plot the predicted versus known *y* variables to gain a
graphical sense for how the model performed:

```{r}
collect_predictions(final_fit) %>% 
    ggplot(aes(x = .pred, y = final_grade)) +
    geom_point()
```

Consider making a modification to the above plot (small or large) using
ggplot2.

### ð§¶ Knit & Check â

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
