---
title: 'Learning Lab 2 Case Study: Interpretation'
author: ""
date: "`r format(Sys.Date(),'%B %e, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
    code_folding: show
    code_download: TRUE
editor_options:
  markdown:
    wrap: 72
bibliography: lit/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

When we left off the first case study, we saw that our model was *pretty
accurate*. We can and will do better in terms of accuracy. But, the
accuracy value we found also raises a broader question: How good was our
model in terms of making predictions?

While accuracy is an easy to understand and interpret value, it provides
a limited answer to the above question about how good our model was in
terms of making predictions.

In this learning and case study, we explore a variety of ways to
understand how good of a predictive model ours is. We do this through a
variety of means:

-   Other **statistics**, such as sensitivity and specificity

-   **Tables**--particularly, a confusion matrix

Our driving question for this case study, then, is: *How good is our
model at making predictions?*

We'll use the [Open University Learning Analytics Dataset
(OULAD)](https://analyse.kmi.open.ac.uk/open_dataset) again--this time,
adding another data source, one on students' performance on assessments.

## Step 0: Loading and setting up

Like in the first learning lab, we'll first load several packages --
{tidyverse}, {tidymodels}, and {janitor}. Make sure these are installed
(via `install.packages()`). Note that if they're already installed, you
don't need to install them again.

```{r}
library(tidyverse)
library(tidymodels)
library(janitor)
```

Like in the code-along for the overview presentation, let's take a look
at the data and do some processing of it.

We'll load the students file together; you'll write code to read the
assessments file, which is named "oulad-assessments.csv". Please assign
the name `assessments` to the loaded assessments file.

```{r}
students <- read_csv("data/oulad-students.csv")
assessments <- read_csv("data/oulad-assessments.csv")
```

#### [Your Turn]{style="color: green;"} ⤵

In the last learning lab, we used the `count()` function. Let's practice
that again, by counting the number of of assessments of different types.
If you need, recall that the data dictionary is
[here](https://analyse.kmi.open.ac.uk/open_dataset). Note what the
different types of assessments mean.

```{r}
assessments %>% 
    count(assessment_type)
```

-   

#### [Your Turn]{style="color: green;"} ⤵

We'll now use another function--like `count()`, from the {tidyverse}.
Specifically, we'll use the `distinct()` function. This returns the
unique (or distinct) values for a specified variable. Learn more about
`distinct()`
[here](https://dplyr.tidyverse.org/reference/distinct.html). Below, find
the distinct assessment IDs.

```{r}
assessments %>% 
    distinct(id_assessment) # this many unique assessments
```

Let's explore the assessments data a bit.

We might be interested in how many assessments there are per course. To
calculate that, we need to `count()` several variables at once; when
doing this, `count()` tabulates the number of unique combinations of the
variables passed to it.

```{r}
assessments %>% 
    count(assessment_type, code_module, code_presentation)
```

Let's explore the dates assignments were submitted a bit -- using the
`summarize()` function:

```{r}
assessments %>% 
    summarize(mean_date = mean(date, na.rm = TRUE), # find the mean date for assignments
              median_date = median(date, na.rm = TRUE), # find the median
              sd_date = sd(date, na.rm = TRUE), # find the sd
              min_date = min(date, na.rm = TRUE), # find the min
              max_date = max(date, na.rm = TRUE)) # find the mad
```

What can we take from this? It looks like, on average, the average (mean
and median) date assignments were due was around 130 -- 130 days after
the start of the course. The first assignment seems to have been due 12
days into the course, and the last 261 days after.

Crucially, though, these dates vary by course. Thus, we need to first
group the data by course. Let's use a different function this time --
`quantile()`, and calculate the first quantile value. Our reasoning is
that we want to be able to act to support students, and if we wait until
after the average assignment is due, then that might be too late.
Whereas the first quantile comes approximately one-quarter through the
semester --- with, therefore, more time to intervene.

```{r}
assessments %>% 
    group_by(code_module, code_presentation) %>% # first, group by course (module: course; presentation: semester)
    summarize(mean_date = mean(date, na.rm = TRUE),
              median_date = median(date, na.rm = TRUE),
              sd_date = sd(date, na.rm = TRUE),
              min_date = min(date, na.rm = TRUE),
              max_date = max(date, na.rm = TRUE),
              first_quantile = quantile(date, probs = .25, na.rm = TRUE)) # find the first (25%) quantile
```

#### [Your Turn]{style="color: green;"} ⤵

Alright, this is a bit complicated, but we can actually work with this
data. Let's use just a portion of the above code, assigning it the name
`code_module_dates`.

```{r}
code_module_dates <- assessments %>% 
    group_by(code_module, code_presentation) %>% 
    summarize(quantile_cutoff_date = quantile(date, probs = .25, na.rm = TRUE))
```

#### [Your Turn]{style="color: green;"} ⤵

Let's take a look at what we just created; type `code_module_dates()`
below:

```{r}
```

What have we created? We found the date that is one-quarter of the way
through the semester (in terms of the dates assignments are due).

#### [Your Turn]{style="color: green;"} ⤵

We can thus use this to group and calculate students' performance on
assignments through this point. To do this, we need to use a join
function --- `left_join()`, in particular. Please use `left_join()` to
join together `assessments` and `code_module_dates`, with `assessments`
being the "left" data frame, and `code_module_dates` the right. Save the
output of the join the name `assessments_joined`.

```{r}
assessments_joined <- assessments %>% 
    left_join(code_module_dates) # join the data based on course_module and course_presentation
```

We're almost there! The next few lines filter the assessments data so it
only includes assessments before our cutoff date.

```{r}
assessments_filtered <- assessments_joined %>% 
    filter(date < quantile_cutoff_date) # filter the data so only assignments before the cutoff date are included
```

Finally, we'll find the average score for each student.

```{r}
assessments_summarized <- assessments_filtered %>% 
    mutate(weighted_score = score * weight) %>% # create a new variable that accounts for the "weight" (comparable to points) given each assignment
    group_by(id_student) %>% 
    summarize(mean_weighted_score = mean(weighted_score)) 
```

As a point of reflection here, note how much work we've done before
we've gotten to the machine learning steps. Though a challenge, this is
typical for both machine learning and more traditional statistical
models: a lot of the work is in the preparation and data wrangling
stages.

Let's copy the code below that we used to process the students data
(when processing the `withdrawn` and `imd_band` variables).

```{r}
students <- students %>% 
    mutate(withdrawn = ifelse(final_result == "Withdrawn", 1, 0)) %>% # creates a dummy code
    mutate(withdrawn = as.factor(withdrawn)) # makes the variable a factor, helping later steps

students <- students %>% 
    mutate(imd_band = factor(imd_band, levels = c("0-10%",
                                                  "10-20%",
                                                  "20-30%",
                                                  "30-40%",
                                                  "40-50%",
                                                  "50-60%",
                                                  "60-70%",
                                                  "70-80%",
                                                  "80-90%",
                                                  "90-100%"))) %>% # this creates a factor with ordered levels
    mutate(imd_band = as.integer(imd_band)) # this changes the levels into integers based on the order of the factor levels
```

#### [Your Turn]{style="color: green;"} ⤵

Finally, let's join together `students` and `assessments_summarized`,
assigning the joined the name `students_and_assessments`.

```{r}
students_and_assessments <- students %>% 
    left_join(assessments_summarized)
```

Big picture, we've added another measure -- another feature -- that we
can use to make predictions: students' performance on assessments 1/4 of
the way through the course.

We're now ready to proceed to our machine learning steps!

## Step 1. Split data

This is identical to what we did in the first learning lab, using the
`students_and_assessments` data. We'll also create a testing data set
we'll use later.

```{r}
set.seed(20230712)

train_test_split <- initial_split(students_and_assessments, prop = .80)
data_train <- training(train_test_split)
data_test <- testing(train_test_split)
```

## Step 2: Engineer features and write down the recipe

We'll also specify the recipe, adding our `mean_weighted_score` variable
we calculated above as well as variables we used in LL1 (case study and
badge). Note how we dummy code three variables using `step_dummy()`
(described further in the first learning lab).

```{r}
my_rec <- recipe(withdrawn ~ disability + imd_band +
                     date_registration + 
                     gender + highest_education + 
                     mean_weighted_score, 
                 data = data_train) %>% 
    step_dummy(disability) %>% 
    step_dummy(gender) %>% 
    step_dummy(highest_education)
```

## Step 3: Specify the model and workflow

These steps are also the same as in LL1.

```{r}
# specify model
my_mod <-
    logistic_reg() %>% 
    set_engine("glm") %>%
    set_mode("classification")

# specify workflow
my_wf <-
    workflow() %>% # create a workflow
    add_model(my_mod) %>% # add the model we wrote above
    add_recipe(my_rec) # add our recipe we wrote above
```

## Step 4: Fit model

Finally, we'll fit our model.

```{r}
fitted_model <- fit(my_wf, data = data_train)
```

Finally, we'll use the `last_fit` function, but we'll add something a
little different - we'll add the metric set here. To the above, we'll
add another common metric - Cohen's Kappa, which is similar to accuracy,
but which accounts for chance agreement.

To do so, we have to specify *which* metrics we want to use using the
`metric_set()` function (see all of the available options
[here](https://yardstick.tidymodels.org/articles/metric-types.html)).
Please specify six metrics in the metric set -- accuracy, sensitivty,
specificity, ppv (recall), npv, and kappa. Assign the name
`class_metrics` to the output of your use of the `metric_set()`
function.

#### [Your Turn]{style="color: green;"} ⤵

```{r}
class_metrics <- metric_set(accuracy, sensitivity, specificity, ppv, npv, kap) # add probs?
```

Then, please use `last_fit`, looking to how we did this in the last
learning lab for a guide on how to specify the arguments. To the
arguments, add `metrics = class_metrics`.

```{r}
final_fit <- last_fit(fitted_model, train_test_split, metrics = class_metrics)
```

We're now ready to move on to interpreting the accuracy of our model.

## Step 5: Interpret accuracy

Let's start with accuracy and a simple confusion matrix.

```{r}
collect_predictions(final_fit) %>% View()

collect_predictions(final_fit) %>% 
    conf_mat(.pred_class, withdrawn)
```

```{r}
collect_metrics(final_fit)
```

The table below elaborates on the meaning of these statistics:

| Statistic           | Interpretation                                                                                   |
|--------------------|----------------------------------------------------|
| Accuracy            | Proportion of the population that is true positive *or* true negative                            |
| True positive (TP)  | Proportion of the population that is affected by a condition and correctly tested positive       |
| True negative (TN)  | Proportion of the population that is not affected by a condition and correctly tested negative   |
| False positive (FP) | Proportion of the population that is not affected by a condition and incorrectly tested positive |
| False negative (FN) | Proportion of the population that is affected by a condition and incorrectly tested positive.    |

Here's where things get interesting: There are other statistics that
have different denominators.

Recall from the overview presentation that we can slice and dice the
confusion matrix to calculate statistics that give us insights into the
predictive utility of the model.

|                                               |                |                                                                                                                                |                                                                                    |           |
|---------------|---------------|---------------|---------------|---------------|
| **Statistic**                                 | **Equation**   | **Interpretation**                                                                                                             | **Question Answered**                                                              | **Value** |
| **Sensitivity** (AKA recall)                  | TP / (TP + FN) | Proportion of those who are affected by a condition and correctly tested positive                                              | Out of all the actual positives, how many did we correctly predict?                |           |
| **Specificity**                               | TN / (FP + TN) | Proportion of those who are not affected by a condition and correctly tested negative;                                         | Out of all the actual negatives, how many did we correctly predict?                |           |
| **Precision** (AKA Positive Predictive Value) | TP / (TP + FP) | Proportion of those who tested positive who are affected by a condition                                                        | Out of all the instances we predicted as positive, how many are actually positive? |           |
| **Negative Predictive Value**                 | TN / (TN + FN) | Proportion of those who tested positive who are not affected by a condition; *the probability that a negative test is correct* | Out of all the instances we predicted as negative, how many are actually negative? |           |

So, what does this hard-won by output tell us? Let's interpret each
statistic carefully in the table below. Please add the value and provide
a *substantive interpretation*. One is provided to get you started.

|                                           |           |                                                                                                                            |
|----------------------|----------------------|----------------------------|
| **Statistic**                             | **Value** | **Substantive Interpretation**                                                                                             |
| Accuracy                                  |           |                                                                                                                            |
| Sensitivity (AKA recall)                  | .997      | The model is remarkably good at doing this; practically all of the students who withdrew were correctly predicted as such. |
| Specificity                               |           |                                                                                                                            |
| Precision (AKA Positive Predictive Value) |           |                                                                                                                            |
| Negative Predictive Value                 |           |                                                                                                                            |
| Cohen's Kappa                             |           |                                                                                                                            |

This process might suggest to you how a "good" model isn't necessarily
one that is the most accurate, but instead is one that has good values
for statistics that matter given our particular question and context.

In [Baker et al.
(2020)](https://www.tandfonline.com/doi/full/10.1080/10824669.2019.1670065),
the authors create a precision-recall graph - one that demonstrates the
trade-off between optimizing these two statistics. Review their paper -
especially the results section - to see the graph they present.

> Baker, R. S., Berning, A. W., Gowda, S. M., Zhang, S., & Hawn, A.
> (2020). Predicting K-12 dropout. Journal of Education for Students
> Placed at Risk (JESPAR), 25(1), 28-54.

We can create a comparable graph for our model, which can help us
understand how we can consider the trade-off between a model that
optimizes between correctly predicting actual students who will withdraw
(recall, or sensitivity) and not falsely predicting as students who will
withdraw those who do not (precision). We'll have to make the
predictions and bind them to our data set.

```{r}
predictions <- fitted_model %>%
  predict(new_data = data_test, type = "prob") # make predictions with our testing data using predict()

data_for_pr_curve <- data_test %>% 
    bind_cols(predictions) # combine the predictions with the data_test data, which includes the known withdrawal status

pr_curve(data_for_pr_curve, withdrawn, .pred_1) %>% 
  # filter(.threshold < 1.01) %>% 
  ggplot(aes(x = recall, y = precision, color = .threshold)) +
  geom_path() +
  coord_equal() +
  theme_bw() +
    scale_color_gradient(low = "white", high = "black")
```

Let's focus on

### 🧶 Knit & Check ✅

Congratulations - you've completed this case study! Consider moving on
to the badge activity next.
