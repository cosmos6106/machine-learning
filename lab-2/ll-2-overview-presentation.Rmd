---
title: "Machine Learning Learning Lab 2: Feature Engineering"
subtitle: "Overview Presentation"
author: "**Dr. Joshua Rosenberg**"
institute: "LASER Institute"
date: '`r format(Sys.time(), "%B %d, %Y")`'
output:
  xaringan::moon_reader:
    css:
     - default
     - css/laser.css
     - css/laser-fonts.css
    lib_dir: libs                        # creates directory for libraries
    seal: false                          # false: custom title slide
    nature:
      highlightStyle: default         # highlighting syntax for code
      highlightLines: true               # true: enables code line highlighting 
      highlightLanguage: ["r"]           # languages to highlight
      countIncrementalSlides: false      # false: disables counting of incremental slides
      ratio: "16:9"                      # 4:3 for standard size,16:9
      slideNumberFormat: |
       <div class="progress-bar-container">
        <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
        </div>
       </div>
---
class: clear, title-slide, inverse, center, top, middle

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, echo=FALSE}
# then load all the relevant packages
pacman::p_load(pacman, knitr, tidyverse, readxl)
```

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringanExtra-clipboard, echo=FALSE}
# these allow any code snippets to be copied to the clipboard so they 
# can be pasted easily
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
```

```{r xaringan-extras, echo=FALSE}
xaringanExtra::use_tile_view()

```

# `r rmarkdown::metadata$title`
----
### `r rmarkdown::metadata$author`
### `r format(Sys.time(), "%B %d, %Y")`

---

# Purpose and Agenda

We have some data and want to develop a prediction model. Supervised machine learning is suited to this aim. In particular, in this learning lab, we explore how we can train a computer to predict students' withdrawal from a course. We use a large data set, the Open University Learning Analytics Dataset (OULAD), focusing on student data at this point. Our model at this point is relatively simple, a generalized linear model.

## What we'll do in this presentation

- Discussion 1
- Key Concepts
- Code-along
- Discussion 2
- Introduction to the other parts of this learning lab

---

# Discussion 1

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---

# Key Concepts

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---

# Code-along

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---

# Discussion 2

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---

# Introduction to the other parts of this learning lab

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---


# Agenda

.panelset[

.panel[.panel-name[Discussion]

- 
- 
-
    
]

.panel[.panel-name[Conceptual Overview]

- 
- 
-

]

.panel[.panel-name[Coding Walkthrough]

- 
- 
- 

]
]

---

# Brief discussion (15-20 seconds each) reflection


.pull-left[
### Eiscuss:

  - Which modality (supervised or unsupervised machine learning) are you interested in using?
  - What data or context are you interested in for your use of ML?
]

.pull-right[

```{r, echo = FALSE, out.width = "75%"}
knitr::include_graphics("img/joro-pointing.jpeg")
```

]

---

# Background

- In the last learning lab, we did a nice job of training a model that was _pretty good_
- But, can we do better?
- This question motivates this learning lab, specifically:
    - Answering it
    - And, developing a _means_ to answer it in a way that does not introduce bias into our analysis
- Feature engineering is a key way we can improve our model
- Feature engineering refers to the part of machine learning wherein we decide which variables to include in which forms

---

# Agenda

.pull-left[
## Part 1: Core Concepts
### Feature engineering
- Selecting and preparing variables for inclusion as features
- k-folds cross-validation
]

.pull-right[

## Part 2: R Code Examples
### Online science classes
- Daa from online science classes
- Interpreting changes in fit measures
]

---

class: clear, inverse, center, middle

# Core Concepts

Let's start with accuracy and a simple confusion matrix.

Let's start with some data from a model - imagine we have a test set with n = 10 values, as follows:

Outcome

Prediction

Correctness of Prediction

1

1

Correct

0

0

Correct

1

1

Correct

1

0

Incorrect

1

1

Correct

0

1

Incorrect

1

1

Correct

0

0

Correct

0

1

Incorrect

1

1

Correct

What's the accuracy of our model? Add your note below:

Now, let's create a confusion matrix based on this data:

{r}
data_for_confusion_matrix <- tibble(Outcome = c(1, 0, 1, 1, 1, 0, 1, 0, 0, 1),
        Prediction = c(1, 0, 1, 0, 1, 1, 1, 0, 1, 1)) %>% 
    mutate(`Correctness of Prediction` = ifelse(Outcome == Prediction, "Correct", "Incorrect"))

data_for_confusion_matrix

Use the tabyl() function (from {janitor} to calculate the accuracy in the code chunk below.

{r}
data_for_confusion_matrix %>% 
    tabyl(`Correctness of Prediction`)

This should accord with your value above.

Now, let's use another function from {tidymodels}, conf_mat(). Pass to that function data_for_confusion_matrix in the code chunk below. First, make Outcome and Prediction both into factors using the mutate() function. Then use conf_mat(), which takes as arguments the names of the outcome and the prediction, unquoted. Learn more here.

{r}
data_for_confusion_matrix %>% 
    mutate(Outcome = as.factor(Outcome),
           Prediction = as.factor(Prediction)) %>% 
    conf_mat(Outcome, Prediction)

This table is super important for interpreting our model. We can associate each of the cells with a statistic. After the colon below, replace the x with the associated number from the confusion matrix.

Row 1, Column 1 – True Positives: (TP) x

Row 1, Column 2 – False Negatives (FN): x

Row 2, Column 1 – False Positives (FP): x

Row 2, Column 2 – True Negatives (TN): x

The table below elaborates on the meaning of these statistics:

Statistic

Interpretation

Accuracy

Proportion of the population that is true positive or true negative

True positive (TP)

Proportion of the population that is affected by a condition and correctly tested positive

True negative (TN)

Proportion of the population that is not affected by a condition and correctly tested negative

False positive (FP)

Proportion of the population that is not affected by a condition and incorrectly tested positive

False negative (FN)

Proportion of the population that is affected by a condition and incorrectly tested positive.

Here's where things get interesting: There are other statistics that have different denominators.

You can think of the following statistics as slicing and dicing the confusion matrix you created above, revealing insights in the process. For the below, please add the values based on the above confusion matrix.

Statistic

Equation

Interpretation

Question Answered

Value

Accuracy

Proportion of the population that is true positive or true negative

Sensitivity (AKA recall)

TP / (TP + FN)

Proportion of those who are affected by a condition and correctly tested positive

Out of all the actual positives, how many did we correctly predict?

Specificity

TN / (FP + TN)

Proportion of those who are not affected by a condition and correctly tested negative;

Out of all the actual negatives, how many did we correctly predict?

Precision (AKA Positive Predictive Value)

TP / (TP + FP)

Proportion of those who tested positive who are affected by a condition

Out of all the instances we predicted as positive, how many are actually positive?

Negative Predictive Value

TN / (TN + FN)

Proportion of those who tested positive who are not affected by a condition; the probability that a negative test is correct

Out of all the instances we predicted as negative, how many are actually negative?

We can also do this automatically. We use the class_metrics() function we creatd above by passing to it our outcome and the prediction, specifying which is the prediction with the estimate argument.

{r}
data_for_confusion_matrix %>% 
    mutate(Outcome = as.factor(Outcome),
           Prediction = as.factor(Prediction)) %>% 
    class_metrics(Outcome, estimate = Prediction)

Recall that the above output is for our data with only 10 rows; that was all a long build-up to be able to interpret our model. We can now calculate the metrics quite easily by running collect_metrics() on our fitted model, final_fit.

{r}
collect_metrics(final_fit)

So, what does this hard-won by output tell us? Let's interpret each statistic carefully in the table below. Please add the value and provide a substantive interpretation. One is provided to get you started.

Statistic

Value

Substantive Interpretation

Accuracy

Sensitivity (AKA recall)

.997

The model is remarkably good at doing this; practically all of the students who withdrew were correctly predicted as such.

Specificity

Precision (AKA Positive Predictive Value)

Negative Predictive Value

Cohen's Kappa



---


class: clear, inverse, center, middle

# Code Examples

*Note how these steps are the same as in the classification example for LL 1*

---

# Let's walk through a few steps

.panelset[

.panel[.panel-name[0]

**Prepare**

```{r, message = FALSE, warning = FALSE, echo = TRUE}
library(tidyverse)
library(tidymodels)

d <- read_csv("data/online-sci-data-joined.csv")
```

]

.panel[.panel-name[1]

**Split data**

```{r panel-chunk-1, echo = TRUE, eval = FALSE}
set.seed(20220712)

train_test_split <- initial_split(d, prop = .80)

data_train <- training(train_test_split)

kfcv <- vfold_cv(data_train, v = 10) # this differentiates this from what we did before
# before, we simple used data_train to fit our model
```
]

.panel[.panel-name[2]

**Engineer features**

```{r, echo = TRUE, eval = FALSE}
recipe(final_grade ~ ., data = data_train) %>% # **edit**: change t final grade
    update_role(student_id, course_id, section, new_role = "ID variable") %>% # this can be any string; for ID variables
    step_normalize(all_numeric_predictors()) %>% # standardizes numeric variables
    step_nzv(all_predictors()) %>% # remove predictors with a "near-zero variance"
    step_dummy(all_nominal_predictors()) %>%  # dummy code all factor variables
    step_novel(all_nominal_predictors()) %>% # add a musing label for factors
    step_impute_knn(all_predictors()) # impute missing data for all predictor variables
```
]

.panel[.panel-name[3]

**Specify the model and workflow**
 
```{r panel-chunk-3, echo = TRUE, eval = FALSE}
my_mod <-
    linear_reg() %>% # note the difference here
    set_engine("lm") %>% 
    set_mode("regression") # and here

# specify workflow
my_wf <-
    workflow() %>%
    add_model(my_mod) %>% 
    add_recipe(my_rec)
```
]

.panel[.panel-name[4]

**Fit model**

```{r panel-chunk-4, echo = TRUE, eval = FALSE}
fitted_model <- fit_resamples(my_wf, resamples = kfcv,
                              control = control_grid(save_pred = TRUE)) # this allows us to inspect the predictions
```
]

.panel[.panel-name[5]

**Evaluate accuracy**

```{r panel-chunk-5, echo = TRUE, eval = FALSE}
fitted_model %>% 
    unnest(.metrics) %>% 
    filter(.metric == "rmse") # we also get another metric, the ROC; we focus just on accuracy for now
```
]
]

---

# What's next?

- **Case study**: You'll run all of this code, focusing on feature engineering and working with resamples
- **Independent practice**: In the independent practice, you'll focus on adding a new (not yet used!) feature engineering step

---

class: clear, center

## .font130[.center[**Thank you!**]]

<br/>
.center[<img style="border-radius: 80%;" src="img/jr-cycling.jpeg" height="200px"/><br/>**Dr. Joshua Rosenberg**<br/><mailto:jmrosenberg@utk.edu>]
